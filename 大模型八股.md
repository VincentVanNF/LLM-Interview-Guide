# 大模型相关知识

## 1. BatchNorm, LayerNorm, RMSNorm

- BatchNorm & LayerNorm: 

  - 公式：$\gamma \odot \frac{x - \hat{\mu}_B}{\sigma_B} + \beta$

  - 都需要进行均值归一化，通过可学习的 **$\gamma$ 缩放参数** 和 **$\beta$ 偏移参数** 调整归一化后的分布，增强表达能力。

  - 区别在于:

    - **BatchNorm对一个Batch内的样本进行归一化**，在样本向量的每个特征维度上进行归一化: 例如在MLP中均值，方差的计算发生在各个特征维度上；在CNN中均值，方差的计算发生在各个通道上。

    - LayerNorm对一个样本的不同序列进行归一化(序列中的不同token计算均值方差)；适用于NLP任务，一个batch内序列长度不一致。

      - **PreNorm**：$x_{t+1} = x_t + F_t(\text{Norm}(x_t))$，先进行Normalization再进行FFN/Attention+残差连接；**更容易训练，效果不如postNorm**

      - **PostNorm**：$x_{t+1} = \text{Norm}(x_t + F_t(x_t))$，进行FFN/Attention+残差连接再进行Normalization

        <img src="https://i-blog.csdnimg.cn/blog_migrate/d4d8a8327721f8368e1bce5f0a1b2096.png" alt="img" style="zoom:67%;" />

- RMSNorm: 优化LayerNorm，不需要计算均值和方差而是计算**均方根(Root Mean Square)**，加快了计算速度。

  - 公式：$\text{RMSNorm}(x) = \gamma \odot \frac{x}{\text{RMS}(x)} \quad \text{where} \quad \text{RMS}(x) = \sqrt{\frac{1}{d} \sum_{x^i \in x} x_i^2 + \varepsilon}$
    - $\gamma$ 是可学习的缩放参数
    - $\varepsilon$ 是防止除零的小常数

## 2. SwiGLU 激活函数

- Swish激活函数: $swish(x) = x \cdot \text{sigmoid}(\beta x)$
- GLU激活函数：门控机制, $\text{GLU}(x) = (x \cdot W + b) \otimes \sigma(x \cdot V + c)$
- SwiGLU激活函数：$\text{SwiGLU}(x) = (xW_u + b_u) \otimes \left[ (xW_v + b_v) \cdot \sigma(xW_v + b_v) \right]$
  - $\beta = 1$
  - 将GLU激活函数中 $\sigma$ 变成 $Swish(xW_v + b_v)$ 实现门控



## 3. KV  Cache

以transformer 官方库中DeepSeek_V3的计算过程举例

-  模型输出：`transformers.models.deepseek_v3.DeepseekV3ForCausalLM()`

  - ```python
    		hidden_states = outputs.last_hidden_state
            # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
            slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
            logits = self.lm_head(hidden_states[:, slice_indices, :])
      
            
            loss = None
            if labels is not None:
                loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
      
                
            return CausalLMOutputWithPast(
                loss=loss,
                logits=logits,
                past_key_values=outputs.past_key_values,
                hidden_states=outputs.hidden_states,
                attentions=outputs.attentions,
            )
    ```

  - 其中 `hidden_states: [batch_size, seq_len, hidden_size]`

  - `logits: [batch_size, logits_to_keep, vocab_size]`

  

- 生成解码阶段: `transformers.generation.utils.GenerationMixin._sample()`库中关于next token prediction的关键代码

  - ```python
    next_token_logits = outputs.logits[:, -1, :].to(copy=True, dtype=torch.float32, device=input_ids.device)
    ```

  -  其中``next_token_logits`的计算过程取的是最后一个token对应的logits进行预测,即对next token的预测是 **将通过多层transformer block的最后一层输出的序列的最后一个token进行预测**

  - 所以实际生成过程中，只需要**计算新的token的Query**，和**历史的 Key，Value**即可计算得到 最后一层输出序列的最后一个token映射到vocab_size的logits分数，从而进行next token prediction

  - 由于用不当前面已经生成的token的Query，所以只需要**缓存历史Key，Value值即可，即为KV Cache**

  - ```python
    # 假设当前有3个token: ["Hello", "world", "!"]
    # 在生成"!"这一步：
    
    Q_new = [Q_!]                    # 只计算新token的Query
    K_all = [K_Hello, K_world, K_!]  # 使用所有token的Key（历史+新）
    V_all = [V_Hello, V_world, V_!]  # 使用所有token的Value（历史+新）
    
    # Attention计算：
    attention_output = softmax(Q_new @ K_all.T) @ V_all
    # 这个输出再经过后续层，最终产生logits
    ```

    

## 4. APE(绝对位置编码) -> RoPE (旋转位置编码)

### APE绝对位置编码

- 公式原理: 
  $$
  PE_t^{(i)} = \begin{cases}
  \sin(w_k t), & if \; i = 2k \\
  \cos(w_k t), & if \; i = 2k + 1
  \end{cases}
  
  \text{这里：}
  
  w_k = \frac{1}{10000^{2k/d_{model}}}
  
  i = 0, 1, 2, 3, \ldots, \frac{d_{model}}{2} - 1
  $$

  - $PE_t^{(i)}$ 表示位置 **$t$** 处第 **$i$** 维的位置编码,该值是一个与token embedding相同维度的向量

  - 偶数维度使用正弦函数，奇数维度使用余弦函数。

  - $w_k$是频率参数，随着$k$增大而减小，**相邻的偶数维度和奇数维度的频率一样**

  - $d_{model}$是模型的隐藏维度

- 优点

  - 随着k的变大，频率会变小，周期变大，尽可能保证位置编码的唯一性

  - 计算快，不需要训练

- 缺点

  - ❌ 不擅长外推到更长的序列
    - 可学习的绝对位置编码无法进行外扩：`self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)`\
    - sin，cos绝对位置编码没有感知相对位置的能力，对于超过训练长度的PE进行attention计算时候出现**分布偏移**

  - ❌ 位置感知弱，只能表达“绝对”位置，无法直接编码词与词之间的相对距离。

### RoPE旋转位置编码

RoPE做的事情实际上就是将输入**向量$x$两两分组，构成2组二维向量，对每组二维向量**，根据其所处的分组编号 $d \in [0, 1, \ldots, \frac{D}{2} - 1]$ **以及当前的位置编号 $m$ 对其旋转一定的角度**

- 核心问题: 找到一个位置编码函数 $f$，**使得位置 $m$ 与 $n$ 的两个embedding经过该位置编码后，进行内积计算时，能够通过一个关于 $m-n$的函数 $g$ 表达**，即通过该编码方式经过attention后仍然能够感知到相对位置关系

- 公式表示：将这种变化用复数表示：
  $$
  \begin{aligned}
  f(q,m) &= R_f(q,m)\,e^{i\Theta_f(q,m)}, \\
  f(k,n) &= R_f(k,n)\,e^{i\Theta_f(k,n)}, \\
  g(q,k,m-n) &= R_g(q,k,m-n)\,e^{i\Theta_g(q,k,m-n)}.
  \end{aligned}
  $$

  - **含义说明：**
    - $f(\cdot), g(\cdot)$：复值函数（值为 $a+ib$ 的形式）。
    - $R_f(\cdot), R_g(\cdot)$：**幅度 / 模长**（非负实数），即 $|f|, |g|$。
    - $\Theta_f(\cdot), \Theta_g(\cdot)$：**相位 / 辐角**（弧度），即 $\arg(\cdot)$。
    - 指数项 $e^{i\Theta} = \cos\Theta + i\sin\Theta$ 表示相位对应的复指数。(欧拉公式)

- 通过证明可以得到 ：**该复数(即该函数变化)可以通过原向量旋转 $m\theta$** 得到，其中m为该token在序列中位置。详细证明过程见苏剑林老师博客: [Transformer升级之路：2、博采众长的旋转式位置编码](https://spaces.ac.cn/archives/8265)

  - ![image-20250813230258980](https://s2.loli.net/2025/08/13/BjKPDga5tdWJCEv.png)

- 将embedding为维度推广到d_model维度：

  - $$
    \begin{pmatrix}
    \cos m\theta_0 & -\sin m\theta_0 & 0 & 0 & \cdots & 0 & 0 \\
    \sin m\theta_0 & \cos m\theta_0 & 0 & 0 & \cdots & 0 & 0 \\
    0 & 0 & \cos m\theta_1 & -\sin m\theta_1 & \cdots & 0 & 0 \\
    0 & 0 & \sin m\theta_1 & \cos m\theta_1 & \cdots & 0 & 0 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & 0 & \cdots & \cos m\theta_{d/2-1} & -\sin m\theta_{d/2-1} \\
    0 & 0 & 0 & 0 & \cdots & \sin m\theta_{d/2-1} & \cos m\theta_{d/2-1}
    \end{pmatrix}
    \begin{pmatrix}
    q_0 \\
    q_1 \\
    q_2 \\
    q_3 \\
    \vdots \\
    q_{d-2} \\
    q_{d-1}
    \end{pmatrix}
    $$

    

  - $$
    \begin{pmatrix}
    q_0 \\
    q_1 \\
    q_2 \\
    q_3 \\
    \vdots \\
    q_{d-2} \\
    q_{d-1}
    \end{pmatrix}
    \otimes
    \begin{pmatrix}
    \cos m\theta_0 \\
    \cos m\theta_0 \\
    \cos m\theta_1 \\
    \cos m\theta_1 \\
    \vdots \\
    \cos m\theta_{d/2-1} \\
    \cos m\theta_{d/2-1}
    \end{pmatrix}
    +
    \begin{pmatrix}
    -q_1 \\
    q_0 \\
    -q_3 \\
    q_2 \\
    \vdots \\
    -q_{d-1} \\
    q_{d-2}
    \end{pmatrix}
    \otimes
    \begin{pmatrix}
    \sin m\theta_0 \\
    \sin m\theta_0 \\
    \sin m\theta_1 \\
    \sin m\theta_1 \\
    \vdots \\
    \sin m\theta_{d/2-1} \\
    \sin m\theta_{d/2-1}
    \end{pmatrix}
    $$

  - 其中 $m$ 代表当前token的序列位置, 两两一组使用相同的 $\theta$, $\theta$计算公式为 $\theta_d = \frac{1}{10000^{2d/D}}$

    - $d$ 越小，即 $x$ 向量的前面部分，$\theta_d$ 就越大也就是三角函数频率越大，这一段为高频段；对于相同的高频段，对位置 $m$ 变化敏感
    - $d$ 越大，即 $x$ 向量的后面部分, $\theta_d$就越小也就是三角函数频率越小，这一段为低频段；对于相同的低频段，对位置 $m$ 变化缓慢

  

##  5. 上下文扩展长度外推：

详细分析参考：[大模型 | 一篇搞明白上下文长度扩展：从RoPE到YARN_rope yarn](https://blog.csdn.net/m0_56255097/article/details/147114526)

假设模型训练只见过[0,4]的序列长度，推理是需要推理[0,8]的序列长度

- 外推：保持相邻点的间隔为1不变，将取值范围从[0,4]直接扩展到[0,8]
- 内插：维持原先的区间不变，从原区间取更多的点来表示新的位置，相邻点之间的间隔从1缩小到0.5

###  Position Interpolation (PI) 线性插值

- **核心思路** 将位置索引按比例“缩小”再输入 RoPE，从而保持训练时的角度分布，避免进入不可见的“外推区域”。

- 数学上，若原始最大训练长度为 $L$，想支持长度 $L^{'}$>$L$ ，则令每个位置 $p$ 的新位置变为 **$p \times \frac{L}{L'}$**

- 相当于把超过训练长度的序列值，压缩到训练时的最大长度内

- **优点**

  - 稳定：避免编码落在训练之外区间，注意力机制更稳定。
  - 易实现：只需修改位置计算。

  **缺点**

  - 模型对非常长的文本外推仍有限：**相当于对$\theta_d$ 进行了放缩  $\theta_{d(PI)} =  \theta_d \times \frac{L}{L'}$,直觉上对相邻位置旋转角度进行缩小，导致原本的高频部分频率变低**，从而损失对高频部分相对位置的学习能力
  - 需微调（如持续预训练），否则效果一般。

### NTK-aware Interpolation

- **核心思路**：传统的 RoPE 扩展(PI)会把全体频率按**固定比例 $s$** 统一“拉伸/压缩”，模型对高频敏感，尽量保持高频部分不变，对低频部分进行插值；即**高频外推，低频内插**

- 公式
  $$
  \begin{aligned}
  & f_{NTK-a}(m,d) = g(m)h_{NTK-a}(\theta_d) \\[2pt]
  & h_{NTK-a}(\theta_d) = s^{\frac{-2d}{D-2}} \theta_d \\[2pt]
  \end{aligned}
  $$

- 其中

  - $ s = \frac{L'}{L}$：扩展比例；
  - $g(m)=m$：保持位置索引不变；

- 直观理解:  **$d=0$ 时，$\theta_d$保持不变，即高频不变直接外推，$d = D/2-1$即最后一组时，直接进行内插，中间过程通过指数函数进行放缩拟合系数**

- 存在问题: **在低频分量部分，即使在训练过程中也没有遇到足够长的序列能够让这部分低频分量经历完一个周期，因此如果直接对这部分进行外推会引入新的旋转角度，导致性能下降**

### YaRN

在NTK-aware Interpolation基础上对 **波长大于训练时最长序列长度（极端低频序列）**部分不能做外推

- 核心思路：在 RoPE 上 **分段（by-parts）插值**以兼顾高/低频，再**给注意力加温度缩放**来稳定长程匹配；

  - NTK-by-parts（分段插值）：**by-parts** 进一步把频率（或等价的波长 $\lambda$）按区间处理：**短波长（足够的高频）完全外推，中间区间既外推也内插，长波长（足够的低频）完全内插**: (个人理解是NTK-aware 只在第一个高频位置做完全外推，最后一个位置做完全内插，而 by-parts **通过波长的方式确定了足够外推的高频位置** 和 **足够完全内插的低频位置** 从而进行分段)

  - 公式

    - $$
      r(d) = \frac{L}{\lambda_d} = \frac{L}{2\pi b^{2d/|D|}} \\[4pt]
      \gamma(r) = \begin{cases}
      0, & r < \alpha \\
      1, & r > \beta \\
      \frac{r - \alpha}{\beta - \alpha}, & \text{otherwise}
      \end{cases} \\[4pt]
      h(\theta_d) = (1 - \gamma(r(d))) \frac{\theta_d}{s} + \gamma(r(d)) \theta_d
      $$

      

    - 直观理解 $\gamma(r)$ 代表 训练序列长度与 $d$ 分量波长的比值，**比值越小**代表此时的 $d$ 分量为**极端低频位置**，**进行完全内插**；**比值越大**代表此时的 $d$ 分量为**极端高频位置**，**进行完全外推**; 两种中间部分**既外推也内插**

  - 注意力“温度”缩放（Attention Scaling）：$\text{softmax}\left(\frac{q^T k}{t\sqrt{|D|}}\right)$

  - 经验上，**随着扩展倍数$s$增大，适度“降温”能稳定困惑度**，直观上**扩展长度越长，温度系数越低**。LLaMA 系列经验公式：$\frac{1}{t} = 0.1 \ln(s) + 1$

- 实践上，YaRN 用很少的长序列微调数据，就把 LLaMA 家族稳定扩到 64k、128k，上下文检索任务（如 passkey）也能保持高准确。

## 6.Attention进阶史（MHA, MQA, GQA, MLA）

进阶解决核心问题: 减少KV Cache

<img src="https://s2.loli.net/2025/08/22/Vm6wsrcoeyzW57i.png" alt="image-20250822223756955" style="zoom: 67%;" />

### MHA (**M**ulti **H**eads **A**ttention）

- 传统的多头注意力机制，每个head 具有独立的query，key，value。
- 参数多，模型表现力强，计算成本高。
- kv cache时需要**保存每个head的key和value**。

### MQA (**M**ulti **Q**uery **A**ttention）

- **不同head的query，共享同一key和value。**
- 参数少，计算快，但牺牲了一定的表达能力。
- kv cache时仅需要缓存一组key，value

### GQA (**G**roup **Q**uery **A**ttention）

- **将不同head的query进行分组，一组query共享同一key和value**
- MHA与MQA的折中方案，平衡了参数计算量和模型表达能力
- kv cache需要保存分组**(大于一，小于head数)**的key，value

### MLA (**M**ulti Head **L**atent **A**ttention)

[MLA详细分析](https://fengyan-wby.github.io/2025/02/13/MLA%EF%BC%88Multi-head-Latent-Attention%EF%BC%89)

![img](https://s2.loli.net/2025/08/22/HKWJScNvIZRFlPb.png)

- 将Q,K,V都先通过矩阵投影到一个更低维的隐空间 $C$，然后再分别投影回hidden_size，**最终在hidden_size空间计算attention**

- MLA的核心是对KV进行压缩后，再送入标准的MHA算法中；

  - **1. 实际缓存的是 K,V共享的 latent vector:** **[$c^{KV}_{t}$ ; $k^{R}_{t}$ ]**
  -  **2. 通过concat解耦旋转位置编码** 和 **矩阵合并** 实现推理加速(**在只缓存K,V共享的 latent vector**的前提下如何将**向上映射回hidden_size的逻辑合并到Q的计算过程中**) 

- 具体实现:

  - K,V

  $$
  \begin{align}
  c_i^{KV} &= W^{DKV} h_i \ \ \ \ \ c_i^{KV} \in \mathbb{R}^{d_c} \\
  k_i^C &= W^{UK} c_i^{KV} \\
  v_i^C &= W^{UV} c_i^{KV}
  \end{align}
  $$

  - Q
    $$
    \begin{align}
    c_i^Q &= W^{DQ} h_t \ \ \ \ \ c_i^Q \in \mathbb{R}^{d'}  \\
    q_i^C &= W^{UQ} c_i^Q
    \end{align}
    $$

  - 其中 $c_i^{KV}$ 为 K,V共享的 latent vector：$c_i^Q$为Q的latent vector;    

  - 对于$q_i^{C\top} k_j^C$的计算，和注意力输出的计算过程
    $$
    \begin{align}
    q_i^{C\top} k_j^C &= (W^{UQ} c_i^Q)^\top W^{UK} c_j^{KV} \\[4pt]
    &= c_i^{Q\top} ((W^{UQ})^\top W^{UK}) c_j^{KV}
    \end{align}
    $$

    $$
    \begin{align}
    u_t &= W^O \sum_{j=1}^{t} \text{Softmax}_j\left(\frac{q_i^{\top} k_j^C}{\sqrt{d_h}}\right) v_j^C \\
    &= W^O \sum_{j=1}^{t} \text{Softmax}_j\left(\frac{q_i^{\top} k_j^C}{\sqrt{d_h}}\right) W^{UV} c_j^{KV} \\
    &= \sum_{j=1}^{t} \text{Softmax}_j\left(\frac{q_i^{\top} k_j^C}{\sqrt{d_h}}\right) (W^O W^{UV}) c_j^{KV}
    \end{align}
    $$

    

  -  将  $(W^{UQ})^\top W^{UK}$ 和 $W^O W^{UV}$ 的矩阵计算过程 **合并在一起作为$Q$的投影矩阵 放在不缓存的 $Q$的计算过程中，因此可以只缓存 $c_i^{KV}$**

  - 旋转位置编码处理：解耦后KV Cache缓存的 Key为 **[$c_j^{KV}$; $k_j^R$]**

    - 加入RoPE逻辑后$(W^{UQ})^\top W^{UK}$ 中间**多了一个与序列相对位置有关的函数$R_{i-j}$,因此不能直接合并为一个固定的参数矩阵最为Q的投影矩阵**

    $$
    \begin{align}
    q_i^{C\top} k_j^C &= (R_i W^{UQ} c_i^Q)^\top \times R_j W^{UK} c_j^{KV} \\
    &= c_i^{Q\top} (W^{UQ})^\top R_i^\top R_j W^{UK} c_j^{KV} \\
    &= c_i^{Q\top} (W^{UQ})^\top R_{i-j} W^{UK} c_j^{KV}
    \end{align}
    $$

    -  MLA采用 **解耦RoPE** 的方式解决这个问题：即**将$Q,K$中的旋转位置编码和序列特征进行 concat拼接在一起，而不是矩阵乘法**

    - 将 $ c_t^{Q\top} M$  和 $(q_{i}^R)^\top$ 视为传统的Q的计算过程
      $$
      \begin{align}
      q_i^R &= RoPE(W^{QR} c_i^Q) \\
      k_j^R &= RoPE(W^{KR} h_j) \\
      q_{i} &= [q_{i}^C; q_{i}^R] \\
      k_{j} &= [k_{j}^C; k_j^R] \\[4pt]
      q_{i}^\top k_{j} &= [q_{i}^C; q_{i}^R]^\top [k_{j}^C; k_j^R] \\
      &= (q_{i}^C)^\top k_{j}^C + (q_{i}^R)^\top k_j^R \\
      &= c_t^{Q\top} M^C c_j^{KV} + (q_{i}^R)^\top k_j^R
      \end{align}
      $$
      

## 7. WordPiece & BPE(Byte Pair Encoding)

### BPE

BPE 每次的迭代目标是找到 **频率最高** 的**相邻字符对**:

1. 根据语料库初始化词汇表 V
2. 统计相邻字符出现的频率
3. 选择频率最高的相邻字符进行合并
4. 更新词汇表V 和 词汇频率
5. 迭代直到大小符合预期或者频率全部为1

**BBPE**：在BPE的基础上在Byte-level为力度进行BPE



### WordPiece

每次的迭代目标是找到 基于**语言模型似然概率的最大值进行合并**

- 假设句子 由n个子词组成，且子词间相互独立 $S = (t_1,t_2,...t_n)$，则句子$S$的**语言模型似然值**等价与所有子词概率乘积：
  $$
  logP(S) = \sum_{i=1}^{n} logP(t_i)
  $$
  
- 将子词$x,y$合并后的似然值变化为：
  $$
  logP(t_z) - (logP(t_x) + logP(t_y)) = log(\frac{P(t_{xy})}{P(t_x)P(t_y)})
  $$

- 即每次进行合并的时候，选取**$\frac{P(t_{xy})}{P(t_x)P(t_y)}$**值最大的子词对进行合并，而不是基于概率





## DPO & PPO & GRPO



### DPO-直接偏好优化

- **去掉reward model**，**直接使用偏好数据对**(一个query，有一个chosen回答和一个reject回答的数据集)进行目标函数建模实现**最大化chosen回答的概率，减小reject回答的概率**
- **通过与reference model的概率比值**，限制模型更新过偏移reference model，学习到某种特定的策略实现最大化奖励导致reward hacking。
- 隐式奖励函数建模：

## CLIP

1. 首先通过两个模态的encoder获取对应的模态embedding
2. 然后进行norm2归一化(用于计算余弦相似度)
3. 对归一化后的embedding进行矩阵乘法获取logits
4. 计算对称的nce_loss

```python
import torch
import torch.nn as nn

class CLIP():
    def __init__(self,config):
        super.__init__(self,config)
        ...
    
    def encode_img(self,img)->torch.Tensor:
        ...
    
    def encode_txt(self,txt)->torch.Tensor:
        ...
       
    def nce_loss(self,logits:torch.Tensor,temperature:float):
        
        label = torch.arange(logits.shape[0],device=logits.device)
        logits = logits / temperature
        
        criterial = nn.CrossEntropyLoss()
        loss = criterial(logits,label)
        return loss
        
    def forward(self,img,txt):
        img_emb = self.encode_img(img)
        txt_emb = self.encode_txt(txt)
        
        img_emb = img_emb / img_emb.norm(dim=-1)
        txt_emb = txt_emb / txt_emb.norm(dim=-1)
        
        logits_img = torch.matmul(img_emb,txt_emb.T) #n,n
        
        loss = (self.nce_loss(logits_img,1) + self.nce_loss(logits_img.T,1)) / 2
        return loss
    
```



## Attention

1. 初始化`hidden_size`，`head_num`, `head_dim`,`wq`,`wk`,`wv`,`wo`
2. 首先计算`query`,`key`,`value`值，并调整shape为`(bsz,head_num,seq,head_dim)`:此时张量不连续
3. 拼接KV Cache：保持`key`,`value`张量的序列长度
4. 计算未归一化的`attention scores`
5. 计算`padding mask`
6. `softmax`归一化 `attention scores`计算得到 `attention probs`
7. 计算attention之后的``context value`,并调整为 `(bsz,seq,hidden_size)`: 注意从非连续张量  `view`为连续张量

```python
class Attention(nn.Module):
    def __init__(self,config):
        super.__init__()
        self.hidden_size = config.hidden_size
        self.head_num = config.head_num
        
        if(self.hidden_size % self.head_num != 0):
            raise Exception("WRONG HEAD NUM")

        self.head_dim = self.hidden_size // self.head_num
        
        self.wq = nn.Linear(self.hidden_size,self.hidden_size)
        self.wk = nn.Linear(self.hidden_size,self.hidden_size)  
        self.wv = nn.Linear(self.hidden_size,self.hidden_size)
        self.wo = nn.Linear(self.hidden_size,self.hidden_size)     
        
    
    def forward(self,query,key,value,mask,past_key_value):
        bsz = query.shape[0]
        seq = query.shape[1]
        
        query = self.wq(query).view(bsz,seq,self.head_num,self.head_dim).transpose(1,2)
        key = self.wk(key).view(bsz,seq,self.head_num,self.head_dim).transpose(1,2)
        value = self.wv(value).view(bsz,seq,self.head_num,self.head_dim).transpose(1,2)
        
        if(past_key_value is not None):
            key = torch.cat((past_key_value[0],key),dim=2)
            value = torch.cat((past_key_value[1],value),dim=2)
        past_key_value = (key,value)
        
        atten_scores = torch.matmul(query,key.transpose(-1,-2)) / torch.sqrt(self.head_dim)
        if(mask is not None):
            mask = mask.unsqueeze(1).unsqueeze(2)
            mask = (1-mask) * (-torch.inf)
            atten_scores = atten_scores + mask
        
        atten_probs = nn.functional.softmax(atten_scores,dim=-1)   
        
        context = torch.matmul(atten_probs,value).transpose(1,2).contiguous().view(bsz,seq,self.hidden_size)
        context = self.wo(context)
  
        return context,past_key_value     
  
```



# LlaMa

## 1

- 去重,过滤低质量数据等操作

- Pre-Norm+RMSNorm (激活函数/残差)
  
- 采用SwiGLU激活函数


- 采用RoPE旋转位置编码:

- 采用KV cache



## 2

- 上下文长度 -> 4096
- GQA 替换传统的 muti-head attention



## 3

- 上下文长度 -> 8192
- 换成openai的tokenizer：tiktoken

## 3.1

- 上下文长度 -> 128k
- 405B超大杯模型
- 多语言模型



# MOE

- 门控网络:输出一个分布，选取topk进行激活对应专家
- 多专家:替换单个FFN为多个专家。



# Deepspeed & Accelerate

1. Deepspeed:
   1. 参数分割策略
      1. 按照模型的层（Layer）进行分割，保留每一层（Layer）为整体，不同层存储在不同的 GPU 中， 多个层（GPU）串行在一起，需要串行执行，这就是所谓的 **流水线并行****（Pipeline Parallel,PP）**。时间效率很差， 并且如果某一层的参数量就很大并超过了单卡的显存就尴尬。当然可以通过异步执行一定程度解决时间效率差的问题，有兴趣的读者可以研读相关资料。
      2. 把参数张量切开，切开张量分开存储很容易，但切开之后，张量计算的时候怎么办？这里可以分两种策略。 1. 张量的计算过程也是可以切割，这样把一个大的张量，切分成多个小张量，每张 GPU 卡只保存一个小片段，每个小张量片段（GPU卡）独立进行相关计算，最后在需要的时候合并结果就行了。这种思路就称为 **张量并行****（Tensor Parallel,TP）** , Megatron 就是走的这个路线。 2. 同样是把参数张量分割，每张卡只保存一个片段。但是需要计算的时候，每张卡都从其他卡同步其它片段过来，恢复完整的参数张量，再继续数据计算。Deepspeed 选取的这个策略，这个策略实现起来更简单一些。
   2. 模型并行:流水线并行，张量并行(不同的参数分割策略)。
      1. 把模型一次完整的计算过程（前后向）分拆到多个 GPU 上进行
   3. 数据并行:pytorch 的 Data Parallel (DP) 和 Distributed Data Parallel (DDP)
      1. 每张卡都能进行模型一次完整前后向计算，只是每张卡处理不同的训练数据批次（batch）。
   4. deepspeed 对参数进行了分割，每张卡存储一个片段，但在进行运算时， 每张卡都会恢复完整的参数张量，每张卡处理不同的数据批次， 因此 deepspeed **属于数据并行**。
   5. 不同级别
      1. ZeRO-0：禁用所有类型的分片，仅使用 DeepSpeed 作为 DDP (Distributed Data Parallel)
      2. ZeRO-1：分割Optimizer States，减少了4倍的内存，通信容量与数据并行性相同
      3. ZeRO-2：分割Optimizer States与Gradients，8x内存减少，通信容量与数据并行性相同
      4. ZeRO-3：分割Optimizer States、Gradients与Parameters，内存减少与数据并行度和复杂度成线性关系。
      5. ZeRO-Infinity是ZeRO-3的拓展。允许通过使用 NVMe 固态硬盘扩展 GPU 和 CPU 内存来训练大型模型。ZeRO-Infinity 需要启用 ZeRO-3。
   6. DeepSpeed提供了多种优化策略和工具,其中ZeRO和Offload是两种重要的技术,它们有以下主要区别:
      1. 优化目标:
         1. ZeRO (Zero Redundancy Optimizer) 主要目标是减少模型状态(参数、梯度、优化器状态)的内存占用。
         2. Offload 则专注于将部分计算和内存负载从GPU转移到CPU或NVMe上。
      2. 实现方式:
         1. ZeRO 通过在数据并行进程间分割模型状态来减少内存占用,有**ZeRO-1、ZeRO-2和ZeRO-3**三个阶段。
         2. Offload 将优化器状态和梯度卸载到CPU内存,同时利用高效的CPU优化器实现。
2. Accelerate
   1. Accelerate是Hugging Face推出的一个库，旨在简化和加速PyTorch模型在多GPU和多机环境下的训练过程。它的主要功能包括：
      1. **多设备支持**：支持单机多GPU、多机多GPU的分布式训练。
      2. **易用性**：提供简单的API，用户只需对原有的训练代码做少量修改，即可实现分布式训练。
      3. **兼容性**：兼容现有的PyTorch生态系统，可以与其他PyTorch库无缝集成。
3. Accelerate和DeepSpeed都是用于分布式训练的框架,但它们在多机多卡训练方面有一些主要区别:
   1. 支持的模型规模:DeepSpeed支持更大规模的模型训练。它提供了更多的优化策略和工具,如ZeRO和Offload,可以处理超大模型.
   2. 配置复杂度:Accelerate的配置相对简单,对大多数模型可以开箱即用。DeepSpeed则需要更详细的配置,但提供了更多的优化选项.
   3. 并行策略:Accelerate主要使用简单的管线并行(Pipeline Parallelism)。DeepSpeed支持更复杂的并行策略,如张量并行(Tensor Parallelism).
   4. 启动方式:使用Accelerate时,通常需要在每台机器上单独启动训练脚本。而DeepSpeed可以使用pdsh工具,只需在一台机器上运行脚本,就能自动将命令和环境变量推送到其他节点.
   5. 性能优化:DeepSpeed提供了更多的性能优化选项,如融合CUDA核函数,可以实现更快的推理速度.
   6. 灵活性:Accelerate对于大多数模型来说更容易使用和适配。DeepSpeed则提供了更多的定制化选项,但可能需要更多的开发工作来适配特定模型.

# Qwen系列



## Qwen  VL

- LLM：Qwen-7B

- ViT：Openclip: ViT-bigG(1.9b)，

  - 使用一个**适配器**压缩图像的token序列长度:一个固定序列长度的(256)可学习的**Query**参数矩阵的cros-attention模块，图像序列作为**Key**。将图像序列压缩到固定的256。

  - 可以感知位置信息，**2d绝对位置编码。**

  - 正弦余弦绝对位置编码向量中，向量中每个元素的位置都是当前 token 的位置(t)

  - ![image-20250307162818379](https://s2.loli.net/2025/03/07/8rQKN6LHmbopDyx.png)

  - 2d绝对位置编码中，位置编码向量维度切割成一半，代表2d的坐标数量。每一办单独做一个1d绝对位置编码。

    ![image-20250307163319761](https://s2.loli.net/2025/03/07/kdy4ap6tnOjvUuP.png)

    

- 训练

  - 第一步冻结LLM训练ViT与适配器 (PT)
  - 第二步训练LLM+ViT+适配器+LLM (PT)
  - 第三部训练适配器+LLM (SFT)

## Qwen2-VL

- 更小的视觉编码器: 675M
- 更多的模型尺寸

- 去掉了适配器，采用原生动态分辨率。为了减少token，采用MLP层进行merge，将2*2的 patch token压缩为1个token。

- 图文数据与视频数据混合训练

  - 采用深度为2的3D卷积，图片会复制一张，但是没有增加图片的patch即token数量，而是相当于将两张图片叠在一起增加了图片每个patch token的embedding维度

- 设计了多模态旋转位置嵌入

  - 2d: 二维坐标获得位置编码，两两一组变为4,4一组

  - 3d:三维坐标获得位置编码,6-6一组进行旋转

    <img src="https://s2.loli.net/2025/03/07/nOAbmH3p9sVI7de.png" alt="image-20250307164852924" style="zoom: 67%;" />



- 训练
  - 训练ViT+MLP，冻结LLM
  - 训练所有
  - 训练LLM与MLP连接层。

## Qwen-2.5-VL

- 与LlaMa对齐:RMSNorm，SwiGLU激活函数
- window attention:只在一个window中做attention
- 绝对时间对齐: 动态FPS采样
  - 将视频三维位置id中的id与绝对时间对齐，而不是与帧对齐。(同一帧id在不同的FPS下代表的是不同的时间，与时间对齐不存在这个问题)
  - 在不同的FPS采样不同的帧，并与绝对时间对齐。



# 大模型面经



# 简历

## clip & attention

```python
import torch.nn as nn
import torch
class Attention(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        self.head_num = args.head_num
        self.hidden_size = args.hidden_size
        if(self.hidden_size % self.head_num != 0):
            raise ValueError("hidden_size should be divided by head_num")
        self.head_dim = self.hidden_size // self.head_num
        
        self.wq = nn.Linear(self.hidden_size,self.hidden_size)
        self.wk = nn.Linear(self.hidden_size,self.hidden_size)
        self.wv = nn.Linear(self.hidden_size,self.hidden_size)
        
        
        
        
    def forward(self,x:torch.Tensor,attention_mask:torch.Tensor):
        # x:[bs,seq,hidden_size],mask:[bs,seq]
        bs = x.shape[0]
        seq_len = x.shape[1]
        attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
        attention_mask = (1-attention_mask)*(-torch.inf)
        
        query = self.wq(x).view(bs,seq_len,self.head_num,self.head_dim)
        key = self.wk(x).view(bs,seq_len,self.head_num,self.head_dim)
        value = self.wv(x).view(bs,seq_len,self.head_num,self.head_dim)
        
        # x.transpose
        attention_score = torch.matmul(
            query.transpose(1,2), #[bs,head,seq,hidden]
            key.transpose(1,2).transpose(-1,-2) #[bs,head,hidden,seq]
        )
        attention_score = attention_score / torch.sqrt(self.hidden_size)
        attention_score = attention_score + attention_mask
        
        attention_probs = nn.functional.softmax(attention_score,-1) #[bs,head,seq,seq]
        
        context_out = torch.matmul(attention_probs,value) #[bs,head,seq,hidden]
        
        context_out = context_out.transpose(1,2).contiguous().view(bs,seq_len,-1)
        
        return context_out
    
    

class Attention_cache(nn.Module):
    def __init__(self,head_num,hidden_size):
        self.head_num = head_num
        self.hidden_size = hidden_size
        
        self.head_dim = self.hidden_size // self.head_num
        
        if(self.hidden_size % self.head_num != 0):
            raise ValueError('wrong head num')     
        
        self.wq = nn.Linear(hidden_size,hidden_size)
        self.wk = nn.Linear(hidden_size,hidden_size)
        self.wv = nn.Linear(hidden_size,hidden_size)
        self.wo = nn.Linear(hidden_size,hidden_size)
        
        
    def forward(self,x:torch.Tensor,attn_mask:torch.Tensor,past_key_value):
        
        # [bs,seq,hidden_size]
        bs = x.shape[0]
        seq_len = x.shape[1]
        
        query = self.wq(x).view(bs,seq_len,self.head_num,self.head_dim).transpose(1,2)
        key = self.wk(x).view(bs,seq_len,self.head_num,self.head_dim).transpose(1,2)
        value = self.wv(x).view(bs,seq_len,self.head_num,self.head_dim).transpose(1,2)
        
        if(past_key_value is not None):
            key = torch.cat((past_key_value[0],key),dim=-2)
            value = torch.cat((past_key_value[1],value),dim=-2)
        past_key_value = (key,value)
        
        attention_scores = torch.matmul(query,key.transpose(-1,-2)) / torch.sqrt(self.hidden_size)
        
        if(attn_mask is not None):
            attn_mask =  attn_mask.unsqueeze(1).unsqueeze(2)
            attn_mask = (1-attn_mask)*(-1e7)
            attention_scores = attention_scores + attn_mask  
            
        attention_probs = nn.functional.softmax(attention_scores,-1)
        
        
        contex_value =  torch.matmul(attention_probs,value)
        contex_value = contex_value.transpose(1,2).contiguous().view(bs,seq_len,-1)
        
        contex_value = self.wo(contex_value)     
         
        return contex_value,past_key_value
```



```python
import torch.nn as nn
import torch


class clip(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.visual = kwargs['visual']
        self.text_embedding = kwargs['text_embedding']
        self.pe = kwargs['pe']
        
    
    def encode_text(self,x):
        '''
            n,d
        '''
        ...
    
    def encode_img(self,x):
        '''
        n,d
        '''
        ...
    def nce_loss(logits):
            '''
            n,n
            '''
            
            deno = logits.exp().sum(-1) #n
            num = torch.diag(logits.exp())
            
            loss = (torch.log(deno) - torch.log(num)).mean()
            return loss
        
    def forward(self,x,y):
        text = self.encode_text(x) #n,d
        img = self.encode_img(y) #n,d
        
        # 计算余弦相似度
        text = text / text.norm(dim=-1)
        img = img / img.norm(dim=-1)
        
        text_matrix = torch.matmul(text,img.T) / self.temp #n,n
        img_matrix = torch.matmul(img,text.T) / self.temp #n,n
         
        loss = (self.nce_loss(text_matrix) + self.nce_loss(img_matrix)) / 2
        

```
