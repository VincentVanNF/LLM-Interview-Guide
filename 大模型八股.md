# 大模型相关知识

## 1. BatchNorm, LayerNorm, RMSNorm

- BatchNorm & LayerNorm: 

  - 公式：$\gamma \odot \frac{x - \hat{\mu}_B}{\sigma_B} + \beta$

  - 都需要进行均值归一化，通过可学习的 **$\gamma$ 缩放参数** 和 **$\beta$ 偏移参数** 调整归一化后的分布，**防止归一化减弱非线性表示，增强表达能力**。

  - 区别在于:

    - **BatchNorm对一个Batch内的样本进行归一化**，在样本向量的每个特征维度上进行归一化: 例如在MLP中均值，方差的计算发生在各个特征维度上；在CNN中均值，方差的计算发生在各个通道上。

    - LayerNorm对一个样本的不同序列进行归一化(序列中的不同token计算均值方差)；适用于NLP任务，一个batch内序列长度不一致。

      - **PreNorm**：$x_{t+1} = x_t + F_t(\text{Norm}(x_t))$，先进行Normalization再**进行FFN/Attention+残差连接**；**更容易训练，效果不如PostNorm(表征能力坍塌)**

      - **PostNorm**：$x_{t+1} = \text{Norm}(x_t + F_t(x_t))$，**进行FFN/Attention+残差连接** 再进行Normalization; **破坏了残差结构，训练困难**

        <img src="https://i-blog.csdnimg.cn/blog_migrate/d4d8a8327721f8368e1bce5f0a1b2096.png" alt="img" style="zoom:67%;" />

- RMSNorm: 优化LayerNorm，不需要计算均值和方差而是计算**均方根(Root Mean Square)**，加快了计算速度。

  - 公式：$\text{RMSNorm}(x) = \gamma \odot \frac{x}{\text{RMS}(x)} \quad \text{where} \quad \text{RMS}(x) = \sqrt{\frac{1}{d} \sum_{x^i \in x} x_i^2 + \varepsilon}$
    - $\gamma$ 是可学习的缩放参数
    - $\varepsilon$ 是防止除零的小常数

## 2. SwiGLU 激活函数

- Swish激活函数: $swish(x) = x \cdot \text{sigmoid}(\beta x)$
- GLU激活函数：门控机制, $\text{GLU}(x) = (x \cdot W + b) \otimes \sigma(x \cdot V + c)$
- SwiGLU激活函数：$\text{SwiGLU}(x) = (xW_u + b_u) \otimes \left[ (xW_v + b_v) \cdot \sigma(xW_v + b_v) \right]$
  - $\beta = 1$
  - **将GLU激活函数中 $\sigma$** 变成 **$Swish(xW_v + b_v)$** 实现门控



## 3. KV  Cache

以transformer 官方库中DeepSeek_V3的计算过程举例

-  模型输出：`transformers.models.deepseek_v3.DeepseekV3ForCausalLM()`

  - ```python
    		hidden_states = outputs.last_hidden_state
            # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
            slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
            logits = self.lm_head(hidden_states[:, slice_indices, :])
      
            
            loss = None
            if labels is not None:
                loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
      
                
            return CausalLMOutputWithPast(
                loss=loss,
                logits=logits,
                past_key_values=outputs.past_key_values,
                hidden_states=outputs.hidden_states,
                attentions=outputs.attentions,
            )
    ```

  - 其中 `hidden_states: [batch_size, seq_len, hidden_size]`

  - `logits: [batch_size, logits_to_keep, vocab_size]`

  

- 生成解码阶段: `transformers.generation.utils.GenerationMixin._sample()`库中关于next token prediction的关键代码

  - ```python
    next_token_logits = outputs.logits[:, -1, :].to(copy=True, dtype=torch.float32, device=input_ids.device)
    ```

  -  其中``next_token_logits`的计算过程取的是最后一个token对应的logits进行预测,即对next token的预测是 **将通过多层transformer block的最后一层输出的序列的最后一个token进行预测**

  - 所以实际生成过程中，只需要**计算新的token的Query**，和**历史的 Key，Value**即可计算得到 最后一层输出序列的最后一个token映射到vocab_size的logits分数，从而进行next token prediction

  - 由于用不当前面已经生成的token的Query，所以只需要**缓存历史Key，Value值即可，即为KV Cache**

  - ```python
    # 假设当前有3个token: ["Hello", "world", "!"]
    # 在生成"!"这一步：
    
    Q_new = [Q_!]                    # 只计算新token的Query
    K_all = [K_Hello, K_world, K_!]  # 使用所有token的Key（历史+新）
    V_all = [V_Hello, V_world, V_!]  # 使用所有token的Value（历史+新）
    
    # Attention计算：
    attention_output = softmax(Q_new @ K_all.T) @ V_all
    # 这个输出再经过后续层，最终产生logits
    ```

    

## 4. APE(绝对位置编码) -> RoPE (旋转位置编码)

### APE绝对位置编码

- 公式原理: 
  $$
  PE_t^{(i)} = \begin{cases}
  \sin(w_k t), & if \; i = 2k \\
  \cos(w_k t), & if \; i = 2k + 1
  \end{cases}
  
  \text{这里：}
  
  w_k = \frac{1}{10000^{2k/d_{model}}}
  
  i = 0, 1, 2, 3, \ldots, \frac{d_{model}}{2} - 1
  $$

  - $PE_t^{(i)}$ 表示位置 **$t$** 处第 **$i$** 维的位置编码,该值是一个与token embedding相同维度的向量

  - 偶数维度使用正弦函数，奇数维度使用余弦函数。

  - $w_k$是频率参数，随着$k$增大而减小，**相邻的偶数维度和奇数维度的频率一样**

  - $d_{model}$是模型的隐藏维度

- 优点

  - 随着k的变大，频率会变小，周期变大，尽可能保证位置编码的唯一性

  - 计算快，不需要训练

- 缺点

  - ❌ 不擅长外推到更长的序列
    - 可学习的绝对位置编码无法进行外扩：`self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)`\
    - sin，cos绝对位置编码没有感知相对位置的能力，对于超过训练长度的PE进行attention计算时候出现**分布偏移**

  - ❌ 位置感知弱，只能表达“绝对”位置，无法直接编码词与词之间的相对距离。

### RoPE旋转位置编码

RoPE做的事情实际上就是将输入**向量$x$两两分组，构成2组二维向量，对每组二维向量**，根据其所处的分组编号 $d \in [0, 1, \ldots, \frac{D}{2} - 1]$ **以及当前的位置编号 $m$ 对其旋转一定的角度**

- 核心问题: 找到一个位置编码函数 $f$，**使得位置 $m$ 与 $n$ 的两个embedding经过该位置编码后，进行内积计算时，能够通过一个关于 $m-n$的函数 $g$ 表达**，即通过该编码方式经过attention后仍然能够感知到相对位置关系

- 公式表示：将这种变化用复数表示：
  $$
  \begin{aligned}
  f(q,m) &= R_f(q,m)\,e^{i\Theta_f(q,m)}, \\
  f(k,n) &= R_f(k,n)\,e^{i\Theta_f(k,n)}, \\
  g(q,k,m-n) &= R_g(q,k,m-n)\,e^{i\Theta_g(q,k,m-n)}.
  \end{aligned}
  $$

  - **含义说明：**
    - $f(\cdot), g(\cdot)$：复值函数（值为 $a+ib$ 的形式）。
    - $R_f(\cdot), R_g(\cdot)$：**幅度 / 模长**（非负实数），即 $|f|, |g|$。
    - $\Theta_f(\cdot), \Theta_g(\cdot)$：**相位 / 辐角**（弧度），即 $\arg(\cdot)$。
    - 指数项 $e^{i\Theta} = \cos\Theta + i\sin\Theta$ 表示相位对应的复指数。(欧拉公式)

- 通过证明可以得到 ：**该复数(即该函数变化)可以通过原向量旋转 $m\theta$** 得到，其中m为该token在序列中位置。详细证明过程见苏剑林老师博客: [Transformer升级之路：2、博采众长的旋转式位置编码](https://spaces.ac.cn/archives/8265)

  - ![image-20250813230258980](https://s2.loli.net/2025/08/13/BjKPDga5tdWJCEv.png)

- 将embedding为维度推广到d_model维度：

  - $$
    \begin{pmatrix}
    \cos m\theta_0 & -\sin m\theta_0 & 0 & 0 & \cdots & 0 & 0 \\
    \sin m\theta_0 & \cos m\theta_0 & 0 & 0 & \cdots & 0 & 0 \\
    0 & 0 & \cos m\theta_1 & -\sin m\theta_1 & \cdots & 0 & 0 \\
    0 & 0 & \sin m\theta_1 & \cos m\theta_1 & \cdots & 0 & 0 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & 0 & \cdots & \cos m\theta_{d/2-1} & -\sin m\theta_{d/2-1} \\
    0 & 0 & 0 & 0 & \cdots & \sin m\theta_{d/2-1} & \cos m\theta_{d/2-1}
    \end{pmatrix}
    \begin{pmatrix}
    q_0 \\
    q_1 \\
    q_2 \\
    q_3 \\
    \vdots \\
    q_{d-2} \\
    q_{d-1}
    \end{pmatrix}
    $$

    

  - $$
    \begin{pmatrix}
    q_0 \\
    q_1 \\
    q_2 \\
    q_3 \\
    \vdots \\
    q_{d-2} \\
    q_{d-1}
    \end{pmatrix}
    \otimes
    \begin{pmatrix}
    \cos m\theta_0 \\
    \cos m\theta_0 \\
    \cos m\theta_1 \\
    \cos m\theta_1 \\
    \vdots \\
    \cos m\theta_{d/2-1} \\
    \cos m\theta_{d/2-1}
    \end{pmatrix}
    +
    \begin{pmatrix}
    -q_1 \\
    q_0 \\
    -q_3 \\
    q_2 \\
    \vdots \\
    -q_{d-1} \\
    q_{d-2}
    \end{pmatrix}
    \otimes
    \begin{pmatrix}
    \sin m\theta_0 \\
    \sin m\theta_0 \\
    \sin m\theta_1 \\
    \sin m\theta_1 \\
    \vdots \\
    \sin m\theta_{d/2-1} \\
    \sin m\theta_{d/2-1}
    \end{pmatrix}
    $$

  - 其中 $m$ 代表当前token的序列位置, 两两一组使用相同的 $\theta$, $\theta$计算公式为 $\theta_d = \frac{1}{10000^{2d/D}}$

    - $d$ 越小，即 $x$ 向量的前面部分，$\theta_d$ 就越大也就是三角函数频率越大，这一段为高频段；对于相同的高频段，对位置 $m$ 变化敏感
    - $d$ 越大，即 $x$ 向量的后面部分, $\theta_d$就越小也就是三角函数频率越小，这一段为低频段；对于相同的低频段，对位置 $m$ 变化缓慢

  

##  5. 上下文扩展长度外推：

详细分析参考：[大模型 | 一篇搞明白上下文长度扩展：从RoPE到YARN_rope yarn](https://blog.csdn.net/m0_56255097/article/details/147114526)

假设模型训练只见过[0,4]的序列长度，推理是需要推理[0,8]的序列长度

- 外推：保持相邻点的间隔为1不变，将取值范围从[0,4]直接扩展到[0,8]
- 内插：维持原先的区间不变，从原区间取更多的点来表示新的位置，相邻点之间的间隔从1缩小到0.5

###  Position Interpolation (PI) 线性插值

- **核心思路** 将位置索引按比例“缩小”再输入 RoPE，从而保持训练时的角度分布，避免进入不可见的“外推区域”。

- 数学上，若原始最大训练长度为 $L$，想支持长度 $L^{'}$>$L$ ，则令每个位置 $p$ 的新位置变为 **$p \times \frac{L}{L'}$**

- 相当于把超过训练长度的序列值，压缩到训练时的最大长度内

- **优点**

  - 稳定：避免编码落在训练之外区间，注意力机制更稳定。
  - 易实现：只需修改位置计算。

  **缺点**

  - 模型对非常长的文本外推仍有限：**相当于对$\theta_d$ 进行了放缩  $\theta_{d(PI)} =  \theta_d \times \frac{L}{L'}$,直觉上对相邻位置旋转角度进行缩小，导致原本的高频部分频率变低**，从而损失对高频部分相对位置的学习能力
  - 需微调（如持续预训练），否则效果一般。

### NTK-aware Interpolation

- **核心思路**：传统的 RoPE 扩展(PI)会把全体频率按**固定比例 $s$** 统一“拉伸/压缩”，**模型对高频敏感，尽量保持高频部分不变**，对低频部分进行插值；即**高频外推，低频内插**

- 公式
  $$
  \begin{aligned}
  & f_{NTK-a}(m,d) = g(m)h_{NTK-a}(\theta_d) \\[2pt]
  & h_{NTK-a}(\theta_d) = s^{\frac{-2d}{D-2}} \theta_d \\[2pt]
  \end{aligned}
  $$

- 其中

  - $ s = \frac{L'}{L}$：扩展比例；
  - $g(m)=m$：保持位置索引不变；

- 直观理解:  **$d=0$ 时，$\theta_d$保持不变，即高频不变直接外推，$d = D/2-1$即最后一组时，直接进行内插，中间过程通过指数函数进行放缩拟合系数**

- 存在问题: **在低频分量部分，即使在训练过程中也没有遇到足够长的序列能够让这部分低频分量经历完一个周期，因此如果直接对这部分进行外推会引入新的旋转角度，导致性能下降**

### YaRN

在NTK-aware Interpolation基础上对 **波长大于训练时最长序列长度（极端低频序列）**部分不能做外推

- 核心思路：在 RoPE 上 **分段（by-parts）插值**以兼顾高/低频，再**给注意力加温度缩放**来稳定长程匹配；

  - NTK-by-parts（分段插值）：**by-parts** 进一步把频率（或等价的波长 $\lambda$）按区间处理：**短波长（足够的高频）完全外推，中间区间既外推也内插，长波长（足够的低频）完全内插**: (个人理解是NTK-aware 只在第一个高频位置做完全外推，最后一个位置做完全内插，而 by-parts **通过波长的方式确定了足够外推的高频位置** 和 **足够完全内插的低频位置** 从而进行分段)

  - 公式

    - $$
      r(d) = \frac{L}{\lambda_d} = \frac{L}{2\pi b^{2d/|D|}} \\[4pt]
      \gamma(r) = \begin{cases}
      0, & r < \alpha \\
      1, & r > \beta \\
      \frac{r - \alpha}{\beta - \alpha}, & \text{otherwise}
      \end{cases} \\[4pt]
      h(\theta_d) = (1 - \gamma(r(d))) \frac{\theta_d}{s} + \gamma(r(d)) \theta_d
      $$

      

    - 直观理解 $\gamma(r)$ 代表 训练序列长度与 $d$ 分量波长的比值，**比值越小**代表此时的 $d$ 分量为**极端低频位置**，**进行完全内插**；**比值越大**代表此时的 $d$ 分量为**极端高频位置**，**进行完全外推**; 两种中间部分**既外推也内插**

  - 注意力“温度”缩放（Attention Scaling）：$\text{softmax}\left(\frac{q^T k}{t\sqrt{|D|}}\right)$

  - 经验上，**随着扩展倍数$s$增大，适度“降温”能稳定困惑度**，直观上**扩展长度越长，温度系数越低**。LLaMA 系列经验公式：$\frac{1}{t} = 0.1 \ln(s) + 1$

- 实践上，YaRN 用很少的长序列微调数据，就把 LLaMA 家族稳定扩到 64k、128k，上下文检索任务（如 passkey）也能保持高准确。

## 6.Attention进阶史（MHA, MQA, GQA, MLA）

进阶解决核心问题: 减少KV Cache

<img src="https://s2.loli.net/2025/08/22/Vm6wsrcoeyzW57i.png" alt="image-20250822223756955" style="zoom: 67%;" />

### MHA (**M**ulti **H**eads **A**ttention）

- 传统的多头注意力机制，每个head 具有独立的query，key，value。
- 参数多，模型表现力强，计算成本高。
- kv cache时需要**保存每个head的key和value**。

### MQA (**M**ulti **Q**uery **A**ttention）

- **不同head的query，共享同一key和value。**
- 参数少，计算快，但牺牲了一定的表达能力。
- kv cache时仅需要缓存一组key，value

### GQA (**G**roup **Q**uery **A**ttention）

- **将不同head的query进行分组，一组query共享同一key和value**
- MHA与MQA的折中方案，平衡了参数计算量和模型表达能力
- kv cache需要保存分组**(大于一，小于head数)**的key，value

### MLA (**M**ulti Head **L**atent **A**ttention)

[MLA详细分析](https://fengyan-wby.github.io/2025/02/13/MLA%EF%BC%88Multi-head-Latent-Attention%EF%BC%89)

![img](https://s2.loli.net/2025/08/22/HKWJScNvIZRFlPb.png)

- 将Q,K,V都先通过矩阵投影到一个更低维的隐空间 $C$，然后再分别投影回hidden_size，**最终在hidden_size空间计算attention**

- MLA的核心是对KV进行压缩后，再送入标准的MHA算法中；

  - **1. 实际缓存的是 K,V共享的 latent vector:** **[$c^{KV}_{t}$ ; $k^{R}_{t}$ ]**
  -  **2. 通过concat解耦旋转位置编码** 和 **矩阵合并** 实现推理加速(**在只缓存K,V共享的 latent vector**的前提下如何将**向上映射回hidden_size的逻辑合并到Q的计算过程中**) 

- 具体实现:

  - K,V

  $$
  \begin{align}
  c_i^{KV} &= W^{DKV} h_i \ \ \ \ \ c_i^{KV} \in \mathbb{R}^{d_c} \\
  k_i^C &= W^{UK} c_i^{KV} \\
  v_i^C &= W^{UV} c_i^{KV}
  \end{align}
  $$

  - Q
    $$
    \begin{align}
    c_i^Q &= W^{DQ} h_t \ \ \ \ \ c_i^Q \in \mathbb{R}^{d'}  \\
    q_i^C &= W^{UQ} c_i^Q
    \end{align}
    $$

  - 其中 $c_i^{KV}$ 为 K,V共享的 latent vector：$c_i^Q$为Q的latent vector;    

  - 对于$q_i^{C\top} k_j^C$的计算，和注意力输出的计算过程
    $$
    \begin{align}
    q_i^{C\top} k_j^C &= (W^{UQ} c_i^Q)^\top W^{UK} c_j^{KV} \\[4pt]
    &= c_i^{Q\top} ((W^{UQ})^\top W^{UK}) c_j^{KV}
    \end{align}
    $$

    $$
    \begin{align}
    u_t &= W^O \sum_{j=1}^{t} \text{Softmax}_j\left(\frac{q_i^{\top} k_j^C}{\sqrt{d_h}}\right) v_j^C \\
    &= W^O \sum_{j=1}^{t} \text{Softmax}_j\left(\frac{q_i^{\top} k_j^C}{\sqrt{d_h}}\right) W^{UV} c_j^{KV} \\
    &= \sum_{j=1}^{t} \text{Softmax}_j\left(\frac{q_i^{\top} k_j^C}{\sqrt{d_h}}\right) (W^O W^{UV}) c_j^{KV}
    \end{align}
    $$

    

  -  将  $(W^{UQ})^\top W^{UK}$ 和 $W^O W^{UV}$ 的矩阵计算过程 **合并在一起作为$Q$的投影矩阵 放在不缓存的 $Q$的计算过程中，因此可以只缓存 $c_i^{KV}$**

  - 旋转位置编码处理：解耦后KV Cache缓存的 Key为 **[$c_j^{KV}$; $k_j^R$]**

    - 加入RoPE逻辑后$(W^{UQ})^\top W^{UK}$ 中间**多了一个与序列相对位置有关的函数$R_{i-j}$,因此不能直接合并为一个固定的参数矩阵最为Q的投影矩阵**

    $$
    \begin{align}
    q_i^{C\top} k_j^C &= (R_i W^{UQ} c_i^Q)^\top \times R_j W^{UK} c_j^{KV} \\
    &= c_i^{Q\top} (W^{UQ})^\top R_i^\top R_j W^{UK} c_j^{KV} \\
    &= c_i^{Q\top} (W^{UQ})^\top R_{i-j} W^{UK} c_j^{KV}
    \end{align}
    $$

    -  MLA采用 **解耦RoPE** 的方式解决这个问题：即**将$Q,K$中的旋转位置编码和序列特征进行 concat拼接在一起，而不是矩阵乘法**

    - 将 $ c_t^{Q\top} M$  和 $(q_{i}^R)^\top$ 视为传统的Q的计算过程
      $$
      \begin{align}
      q_i^R &= RoPE(W^{QR} c_i^Q) \\
      k_j^R &= RoPE(W^{KR} h_j) \\
      q_{i} &= [q_{i}^C; q_{i}^R] \\
      k_{j} &= [k_{j}^C; k_j^R] \\[4pt]
      q_{i}^\top k_{j} &= [q_{i}^C; q_{i}^R]^\top [k_{j}^C; k_j^R] \\
      &= (q_{i}^C)^\top k_{j}^C + (q_{i}^R)^\top k_j^R \\
      &= c_t^{Q\top} M^C c_j^{KV} + (q_{i}^R)^\top k_j^R
      \end{align}
      $$
      

## 7. 大模型分词: WordPiece & BPE(Byte Pair Encoding)

1. 数据量足够大的情况下: **词表(vocabulary越大)**，压缩率越高(单个token包含的信息越多)，效果越好。
2. 太大的 vocabulary 需要做一些训练和推理的优化，所以要**平衡计算和效果。**

### BPE

BPE 每次的迭代目标是找到 **频率最高** 的**相邻字符对**:

1. 根据语料库初始化词汇表 V
2. 统计相邻字符出现的频率
3. 选择频率最高的相邻字符进行合并
4. 更新词汇表V 和 词汇频率
5. 迭代直到大小符合预期或者频率全部为1

**BBPE**：在BPE的基础上在Byte-level为力度进行BPE



### WordPiece

每次的迭代目标是找到 基于**语言模型似然概率的最大值进行合并**

- 假设句子 由n个子词组成，且子词间相互独立 $S = (t_1,t_2,...t_n)$，则句子$S$的**语言模型似然值**等价与所有子词概率乘积：
  $$
  logP(S) = \sum_{i=1}^{n} logP(t_i)
  $$
  
- 将子词$x,y$合并后的似然值变化为：
  $$
  logP(t_z) - (logP(t_x) + logP(t_y)) = log(\frac{P(t_{xy})}{P(t_x)P(t_y)})
  $$

- 即每次进行合并的时候，选取**$\frac{P(t_{xy})}{P(t_x)P(t_y)}$**值最大的子词对进行合并，而不是基于概率



##  8. FFN的作用

- Attention, FFN, ResNet三者去掉任何一个transformer模型都会变得不可用
  - Attention 进行信息的提取和聚合，残差连接提供信息宽带，提升网络深度。
  - FFN通过线性变化(升维之后降维) 学习高纬度的抽象语义知识和记忆能力，通过非线性激活函数提升模型表达能力同时创造了一种稀疏性。



## 9. Weight Tying

- 在大模型参数矩阵中，最前面有一个`embedding`矩阵:用于将 离散的token表示（one-hot）映射到 连续的向量空间，大小为: $U \in R^{V \times H}$，其中$V$为词表的大小，$H$为大模型的``hidden_size`
- 在最终进行预测输出，`softmax`之前 也有一个线性层，用于将大模型的向量空间映射回词表空间从而进行基于词表的概率预测进行next token prediction，大小为: $U' \in R^{H \times V}$
- 在不考虑`bias`的情况下，两个参数矩阵的大小是相同的，**Weight Tying** 的操作即将两个参数矩阵共享
  - 参数共享可以减小大模型的参数量
  - $U$在反向传播中不如$V$训练得充分。将两者绑定在一起，缓和了这一问题，可以训练得到质量更高的新矩阵。（个人理解是$V$矩阵直接连接到最终的预测部分和损失，并且由于梯度反向传播在最前面受到影响而$U$在梯度反向传播时影响最弱）



## 10. Lora

<img src="https://s2.loli.net/2025/04/17/iLzhyvVERecl9Hn.png" alt="img" style="zoom:67%;" />

- 其中蓝色部分为预训练权重，而红色部分为微调的增量权重。因为微调可以理解为在冻结的预训练权重的基础上加上微调过程中产生的权重更新量。表示为$\Delta W$，其中$\Delta W\in\mathbb{R}^{d*d}$
- 对于一个下游任务，如果知道$\Delta W$，则可以通过**奇(qi)异值分解**（SVD）拆分为两个低秩矩阵的乘积:$\Delta W = BA$,$\mathbf{A} \in \mathbb{R}^{r \times d},  \mathbf{B} \in \mathbb{R}^{d \times r}$
  - **秩表示的是矩阵的信息量**。如果矩阵中的某一维，总可以通过其余维度线性推导而来，那么对模型来说，这一维的信息是冗余的，是重复表达的。矩阵的秩表示**矩阵中线性无关行或列的最大数目**。秩反映了矩阵中信息的多少，或者说是矩阵的“维数”。因此全参微调权重$\Delta W $中可能存在冗余信息，并不需要完整的d*d的尺寸来表示，需要找出其中真正有用的特征维度。使用SVD可以解决这个问题。
- $A,B$分别为两个低秩矩阵，其中r被称为秩，对$A$做高斯初始化。对$B$做零初始化。却对于两个低秩矩阵，会使用超参 α 进行scaling。即**$h=Wx+\frac{\alpha}{r}BAx$** 一般α > r，例如α =32，r=4。至于为什么给B初始化为0，是为了在刚开始训练时不会给模型带来额外的噪声，而对$A$和对$B$进行初始化，作者并未发现有明显差别。



## 11. 信息量，熵，交叉熵，KL散度

- **信息量**: $I(x)=-\mathrm{log}P(x)$ 代表一个事件发生所带来的信息增益,事件发生的概率越低，信息量越大，因为越出乎意料。
- **熵:** $H(X)=-\sum_{i=1}^nP(x_i)\mathrm{log}P(x_i)$ 度量一个随机变量的不确定性或信息量。 是信息量的期望值，**熵越大，随机变量越不可预测**。**如果事件确定，则熵为0**.
- **KL散度**: $D_{KL}(P\parallel Q)=\sum_xP(x)\mathrm{log}\frac{P(x)}{Q(x)}$ 度量的是**使用分布Q来近似分布P导致的额外信息损失**，**表示Q偏离P的程度**
  - 非对称，因此不是一个真正的度量距离。
  - 非负
- **交叉熵**: $H(P,Q)=-\sum_xP(x)\mathrm{log}Q(x)$ 度量**真实分布于预测分布之间**的差异
- 交叉熵与KL散度的关系:
  - $H(P,Q)=H(P)+D_{KL}(P\parallel Q)$
  - 交叉熵 = 真实分布熵 + KL散度，即等于自身不确定性 + 用分布Q近似分布P时额外引入的不确定性
  - 在机器学习中最小化交叉熵等价于最小化KL散度，因为H(P)是一个确定的值



## 12. 激活函数Sigmoid，Tanh，ReLu，LeakyReLU, PReLU（Parametric Relu）, RReLU

![image-20250830184904492](https://s2.loli.net/2025/08/30/1bcqiP8nYCkUwAx.png)

为什么需要激活函数: 首先数据的分布绝大多数是非线性的，而一般神经网络的计算是线性的，引入激活函数，是在神经网络中引入非线性，强化网络的学习能力。所以激活函数的最大特点就是非线性。

### sigmoid

- sigmoid 函数和导数分别为：
  $$
  \begin{align}
  \sigma(z) &= \frac{1}{1 + e^{-z}} \\
  \sigma'(z) &= \sigma(z) \cdot (1 - \sigma(z))
  \end{align}
  $$

  - 梯度消失：$\text{当 }\sigma(z) \to 1 \text{ 时，右侧的 }  (1 - \sigma(z)) \to 0, \text{当 } \sigma(z) \to 0 \text{ 时，左侧的 } \sigma(z) \to 0$

  - 激活函数计算量大：在正向传播和反向传播中都包含幂运算和除法）

  - 非以0为中心的输出：Sigmoid的输出不是0均值（即zero-centered）:这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入，随着网络的加深，会改变数据的原始分布。

    

### tanh

- 公式：
  $$
  \begin{align}
  \tanh(x) &= \frac{e^x - e^{-x}}{e^x + e^{-x}} \\
  \tanh(x) &= \frac{2}{1 + e^{-2x}} - 1
  \end{align}
  $$

  - tanh的输出范围时(-1, 1)，解决了Sigmoid函数的**不是zero-centered输出问题**；
  - 梯度消失: tanh导数范围在(0, 1)之间，相比sigmoid的(0, 0.25)，梯度消失问题会得到缓解，但仍然还会存在



### ReLu

- 公式：
  $$
  ReLu(x)=max(0, x)
  $$

  - 相比Sigmoid和tanh，ReLU摒弃了复杂的计算，提高了运算速度。
  - ReLU的有效导数是常数1：
    - 解决了深层网络中出现的梯度消失问题，
    - 但要防范ReLU的梯度爆炸
  - 将神经元置0：
    - 使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。
    - 容易得到更好的模型，但也要防止训练中出现模型‘Dead’情况。

### Leaky ReLU, PReLU（Parametric Relu）, RReLU（Random ReLU）

![image-20250830184928213](https://s2.loli.net/2025/08/30/MSsWoYXmFCipbvG.png)

- 为了防止模型的‘Dead’情况，后人将x<0部分并没有直接置为0，而是给了一个很小的负数梯度值 $\alpha$
- **Leaky ReLU**：为常数，一般设置 0.1/0.01。这个函数通常比 Relu 激活函数效果要好，但是效果不是很稳定
- **PRelu（参数化修正线性单元）**：作为一个可学习的参数，会在训练的过程中进行更新。
- **RReLU（随机纠正线性单元）**：负值的斜率在训练时是随机的，在测试中是固定的



## 13. Bert & GPT

### Bert: **B**idirectional **E**ncoder **R**epresentations from **T**ransformers

- encoder-only，bidirectional attention进行预训练。
- 预训练时使用 **M**ask **L**anguage **M**odel任务，即mask掉部分token，模型根据前后token预测mask掉的token。
- **处理一词多义**: bidirectional attention加上MLM任务，使得每个token能够注意到上下文的token来得到自己embedding。
- 下游任务: 整个encoder的**最后一层的[CLS]**学到的向量可以很好地作为整句话的语义表示，从而适配一些setence层面的任务，如整句话的情感分类。
  - 文本分类
  - 情感分析
  - 句子相似度比较
  - 问答系统

### GPT: **G**enerative **P**re-trained **T**ransformer

- decoder-only，casual attention进行预训练。
- 预训练时使用 **N**ext **T**oken **P**rediction任务，decoder生成的时候只能看到之前的token。
- 下游任务
  - 文本生成
  - 翻译
  - 对话
  - 摘要



## 14. L1，L2 正则化/权重衰减(weight decay)

P-范数：$\|x\|_p := \left(\sum_{i=1}^{n} |x_i|^p\right)^{1/p}$

- L1-范数: $\|x\|_1 := \sum_{i=1}^{n} |x_i|$
- L2-范数: $\|x\|_2 := \left(\sum_{i=1}^{n} |x_i|^2\right)^{\frac{1}{2}} = \sqrt{x_1^2 + \cdots + x_n^2}$

### 正则化/权重衰减

在损失函数中加入了L1，L2范数：
$$
\begin{align}
L(\theta) &= L_{\text{original}}(\theta) + \lambda \sum_{i=1}^{n} |\theta_i| \\
L(\theta) &= L_{\text{original}}(\theta) + \lambda \sum_{i=1}^{n} \theta_i^2
\end{align}
$$

- L1-Norm: 让权重参数**变得稀疏，部分直接变成0**。起到一定选择特征的作用。
- L2-Norm: 使权重更新更加平滑，所有特征都能在一定程度上参与模型训练。
- 在SGD中**权重衰减即为L2正则化**，在其他例如AdamW中则不是。



## 15. 优化器: SGDM & Adagrad & RMSProp & Adam & AdamW

### SGDM

- 相比于简单的梯度下降(SGD)，引入**momentum动量** (带方向的矢量) 防止模型训练时进入**鞍点**或**局部最优**

- 初始化:
  $$
  \begin{align}
  &\theta_0 \text{（初始参数）} \\
  &m_0 = 0 \text{（初始动量为0）} \\
  \end{align}
  $$

- 迭代过程：
  $$
  \begin{align}
  \text{1. 计算梯度：} g_t &= \nabla_{\theta} L(\theta_t) \\
  \text{2. 更新动量：} m_{t+1} &= \lambda m_t + \eta g_t \\
  \text{3. 更新参数：} \theta_{t+1} &= \theta_t - m_{t+1} \\
  \end{align}
  $$

  - $ \lambda$ 为 动量系数
  - $\eta$ 为学习率

### Adagrad

- 引入了**自适应学习率**：对于参数矩阵的不同参数 $\theta^i$，使用不同的学习率

- 公式:
  $$
  \theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\sum_{i=0}^{t-1}(g_i)^2}} g_{t-1}
  $$

  - 之前的梯度越大(优化方向越陡，学习率变小；反之。
  - $\eta$ 为学习率
  - 对于每个参数的学习率，除以之前 (t-1)次迭代的梯度平方和开根号

  

### RMSProp

- 引入迭代衰减

- 公式
  $$
  \begin{align}
  \theta_t &= \theta_{t-1} - \frac{\eta}{\sqrt{v_t}} g_{t-1} \\
  v_1 &= g_0^2 \\[4pt]
  v_t &= \alpha v_{t-1} + (1 - \alpha)(g_{t-1})^2 \\
  \end{align}
  $$

  -  $\alpha$ 为衰减系数，越靠近当前阶段的梯度影响越大，更合理。
  - $\eta$ 为学习率

  

### Adam & AdamW

- Adam: SGDM + RMSProp(动量 + 自适应学习率)

  - 公式如下:
    $$
    \begin{align}
    m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t  \\
    v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
    \hat{m}_t &= \frac{1}{1 - \beta_1^t} m_t \\
    \hat{v}_t &= \frac{1}{1 - \beta_2^t} v_t \\
    \theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \varepsilon} \hat{m}_t
    \end{align}
    $$
    

- AdamW：将权重衰减项**从梯度的计算中**，直接加在了最后的**权重更新步骤上**。

  - 在Adam中，使用L2-Norm进行权重衰减:
    -  $L(\theta) = L_(\theta) + \frac{\lambda }2 \sum_{i=1}^{n} \theta_i^2$
    - 梯度：$g_t = \nabla_{\theta} L(\theta_{t-1}) + \lambda \theta_{t-1}$
  - 由于Adam中使用了自适应学习率和动量，且分别具有衰减缩放系数，**受到衰减系数影响直接在梯度计算时加入参数衰减并不等价于直接对权重进行衰减**
  - AdamW直接在最后**权重更新上加上权重衰减**：
    - $\theta_t = \theta_{t-1} - \alpha \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \varepsilon} + \lambda \theta_{t-1} \right)$



## 16.Flash Attention

- 内存访问效率：普通的attention其内存访问量是$O(N^2)$, 而FlashAttention是亚二次 / 接近线性。

- 准确性：FlashAttention并不是Attention机制的相似计算，而是通过算法实现完全一致的结果。

- IO Aware:：FlashAttention会更加注重GPU的特性，而不是当作黑盒。

- 前提：

  - GPU的计算能力(FLOPS)比内存吞吐量快：因此由于木桶效应，涉及到计算能力和内存吞吐合作串行的流程时，**瓶颈在于内存的吞吐效率上**

    <img src="https://s2.loli.net/2025/08/30/URCP5kKd2vnEZge.png" alt="img" style="zoom: 50%;" />

    -  对于一个操作可分为

      - 计算受限型(比如矩阵乘法matmul)
      - 内存受限型(比如activation, **dropout**, masking等element-wise操作和**softmax**, layer norm, sum等reduction操作)

    - 实际Attention计算的时间消耗: **大部分时间是内存首先型操作**，**证明确实瓶颈在于内存的吞吐效率上**

      <img src="https://s2.loli.net/2025/08/31/WblusARJqCUh1gX.png" alt="img" style="zoom: 33%;" />

  -  GPU的架构也是分层架构：类似与CPU的寄存器与内存的分层结构

    <img src="https://fancyerii.github.io/img/fa/5.png" alt="img" style="zoom: 33%;" />

    - 核心点在于需要如何高效的使用SRAM，减少HDM与SRAM之间的通信消耗

  - 原始Attention计算过程: 整个过程**读写了HBM四次**，且存储了**完整的Attention分数矩阵**

    <img src="https://s2.loli.net/2025/08/31/K392fN7bgazhFrV.png" alt="img" style="zoom: 50%;" />

- FlashAttention的核心优化是将4次的HBM访存操作减少到一次

  - **分块处理（Tiling）**：**在前向和后向传递中使用**，简单讲就是将$N*N$的 softmax 分数矩阵划分为块。

  - **重新计算**： **仅在后向传递中使用** 

  - **算法原理**: 假设一个向量 $x \in \mathbb{R}^{2B}$,将其分成两块，每一块中有$B$个元素，

    - 块内：当前每个块的向量大小为B，$x = [x_1,...,x_B]$
      $$
      \begin{align}
      m(x) &= \max_i(x_i) \\
      f(x) &= [e^{x_1-m(x)}, \ldots, e^{x_B-m(x)}] \\
      l(x) &= \sum_i f(x)_i \\
      softmax(x) &= \frac{f(x)}{l(x)}
      \end{align}
      $$

      -  其中$m(x)用于$寻找块内的最大值，**softmax计算过程中，每个元素减去一个固定不会影响最终结果(相当于分子分母同时乘一个系数)**，**每一项减去$m(x)$使softmax计算更加稳定不会溢出**
      - $f(x)$为分子项，$l(x)$为分母项的**块内求和**

    - 块间：**合并两个块$x^{(1)}$与$x^{(2)}$**为最终的softmax结果
      $$
      \begin{align}
      m(x) &= m([x^{(1)}, x^{(2)}]) = \max(m(x^{(1)}), m(x^{(2)})) \\
      f(x) &= [e^{m(x^{(1)})-m(x)} f(x^{(1)}), e^{m(x^{(2)})-m(x)} f(x^{(2)})] \\
      l(x) &= l([x^{(1)}, x^{(2)}]) = e^{m(x^{(1)})-m(x)} l(x^{(1)}) + e^{m(x^{(2)})-m(x)} l(x^{(2)}) \\
      softmax(x) &= \frac{f(x)}{l(x)}
      \end{align}
      $$

      -  其中$m(x)$代表合并后的**大块的最大值**
      - $f(x)$ 与 $l(x)$更新：第一块的系数 $e^{m(x^{(1)})-m(x)}$，第二块的系数 $e^{m(x^{(2)})-m(x)}$；作用将每个元素都变为 $x_i-m(x)$
        - 假如$m(x^{(1)}) > m(x^{(2)})$，
          - 块1中的元素:已经是$x_i-m(x)$,因此$f(x^{(1)})$和$l(x^{(1)})$的系数都是1，
          - 块2中的元素：$ e^{m(x^{(2)})-m(x)} * e^{x-m(x^{(2)})} = e^{x-m(x)}$
      - 因此最终的softmax实现了在合并的块中的softmax计算

  - 算法流程：对于$\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{N \times d}$ **存储在HBM中**，SRAM的大小为$M$，$QK^{T}  \in \mathbb{R}^{N \times N}$

    1. 设置块的行大小 $B_r = \frac{M}{4d}$, 块的列大小为 $B_c = \min(\frac{M}{4d}, d)$ min函数的目的是防止块大小

       $B_r \times B_c > M/4$,这样就无法把**4个这样块**放到VRAM里。

    2. 初始化：$\mathbf{O} = (0)_{N \times d} \in \mathbb{R}^{N \times d}, \ell = (0)_N \in \mathbb{R}^N, m = (-\infty)_N \in \mathbb{R}^N \text{ in HBM}$

       对于每一行来说，$l$ 是一个标量，用于累加指数和，由于输出有$N$行，所以这里的是长度为$N$向量。

       $m$用于记录每一行当前最大的值，所以也是长度为$N$，而$-inf$是求$max$的合适初始值。

    3. 把 $Q \in \mathbb{R}^{N \times d}$ 切分成 $T_r = \left\lceil \frac{N}{B_r} \right\rceil$ 个大小为$B_r \times d$ 的块，把K和V切分成 $T_c = \left\lceil \frac{N}{B_c} \right\rceil$ 个大小为

       $B_c \times d$的块。

       因此每次计算$QK^T V$ 是 $B_r \times d$ 的 $Q_i $和 $d \times B_c$ 的 $K_j^T$ 和 $B_c \times d$ 的 $V_j$

       这样得到的最终大小是 $(B_r \times d) \times (d \times B_c) \times (B_c \times d) = (B_r \times d)$

    4. 根据前面的计算，结果矩阵$O$需要切分成} $B_r \times d$的块来存放中间结果。长度为$N$的 $l$ 和 $m$ 也要切分成$B_r$\个元素的块，用于存放这些行当前的指数累加值。

    5. 循环：首先外层循环取出大小为**$ d \times B_c$ 的$K_j^T$ 和大小为 $B_c \times d$ 的$V_j$**，然后内层循环遍历整个$Q$，比如当是i，也就是大小为**$B_r \times d$的$Q_i$**：K的分块决定的是attention矩阵每一行的分块(**每一个softmax计算分成几块**)，Q的分块决定的attention矩阵中每一列的分块(**几行为一块**)。

       可以计算$O = softmax(Q_i K_j^T V_j)$，不过这是部分的计算结果，所以要保存(更新)中间统计量**$m$和$l$** 等到$j+1$的下一次循环时，内层循环还会再次遍历$Q$那个时候会计算$O = softmax(Q_i K_j^T V_j)$，然后把这次的结果合并到最终的结果里，包括统计量也需要同步更新

       <img src="https://s2.loli.net/2025/08/31/Liz2G9Y1JTdjo8E.png" alt="img" style="zoom: 50%;" />

       1. 外层分块循环($K,V$) 
       2. 把**$Kj$和$Vj$从HBM加载到SRAM**。根据前面的讨论，这会占据 **SRAM 50%的存储。**
       3. 内层分块循环($Q$)
       4. 把$Q_i(B_r×d)$和$O_i(B_r×d)$加载进SRAM，同时把$l_i(Br)和m_i(Br)$也加载进去。**$Q_i$和$O_i$会占据另一半的显存**。而$l_i$和$m_i$比较小，根据[论文作者的说法](https://github.com/Dao-AILab/flash-attention/issues/618)可以放到寄存器里。

    6. 计算分块矩阵$Q_i(B_r \times d)$ 和 $K_j$的转置$(d \times B_c)$的乘积，得到$score=S_{ij}(B_r \times B_c)$。

       这里不需要计算$N \times N$的完整$S$矩阵，只需要很小的$S_{ij}$

       <img src="https://s2.loli.net/2025/08/31/dCDTZ7BMFIf9PNl.png" alt="img" style="zoom:33%;" />

    7. 根据算法原理中对分块softmax的计算与合并，对 **不同行的不同softmax块的$m$参数和$l$参数进行更新**：如图此时需要根据保存的绿色块中每行的 $\tilde{m}$ 与当前黄色块的 $m$ 根据块间的更新计算公式，更新$m$ 与 $l$

       <img src="https://s2.loli.net/2025/08/31/czGMdNpkoJZt2a9.png" alt="img" style="zoom: 33%;" />

- 额外的空间：需要**$O(N)$**的空间来存储累计量$l$,$m$,而**不需要存储完整的$N*N$的矩阵。**

- 整个计算过程中，都是在SRAM中进行的，仅在开始和结束的时候进行读取HBM操作，**大大降低了访存时间**

- 计算量仍然为$O(N^2)$，并且无法**返回完整的attention矩阵**



## 17. ROC曲线 & AUC值

### ROC曲线

- X坐标：FPR(伪正率):**所有负例样本**中被模型**预测为正例**的比率

- X坐标：TPR(真正率):**所有正例样本**中被模型**预测为正例**的比率
- 使用不同的阈值得到不同的(FPR,TPR)绘制的曲线叫做ROC曲线
- 当阈值为1时，所有结果预测为错误，经过(0,0)；当阈值为0时，所有结果预测为正确经过(1,1)



### AUC值

Area Under Curve，定义为ROC曲线下的面积

- 使用AUC值作为评价标准：是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，**对应AUC更大的分类器效果更好**。
- **AUC的含义为**：**正样本分数高于负样本的概率**
- 从AUC判断分类器（预测模型）优劣的标准：
  - AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
  - 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
  - AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
  - AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

## 18. 衡量大模型评估指标

### 困惑度(Perplexity)

对于一个测试序列 w₁, w₂, ..., wₙ，困惑度定义为：

$$
\text{PPL} = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i|w_1,\ldots,w_{i-1})\right)
$$
其中：

- $N$ 是序列长度
- $P(w_i|w_1,\ldots,w_{i-1})$ 是在给定前文条件下预测第$i$个词的概率

直观理解:

- **困惑度越低**：模型对数据越"不困惑"，预测能力越强
- **困惑度越高**：模型越"困惑"，预测能力越弱
- 困惑度可以理解为：平均而言，模型在每个位置需要在多少个等概率选择中做决定
- 交叉熵的指数：交叉熵损失越低，困惑度越低；







### BLEU 分数

- 常用于机器翻译，参考译文质量的指标

- 公式：
  $$
  \text{BLEU-N} = \text{BP} \cdot \exp\left(\frac{1}{N} \sum_{n=1}^{N} \log p_n\right)
  $$

  - $\text{BP}$ 是简洁性惩罚 (Brevity Penalty)：当模型输出比参考文本长时BP=1,不需要惩罚。
    $$
    \text{BP} = \begin{cases}
    1 & \text{if } c > r \\
    e^{(1-r/c)} & \text{if } c \leq r
    \end{cases}
    $$

    - $c$ 是候选译文的长度
    - $r$ 是最接近候选译文长度的参考译文长度

  - $N$ 是考虑的最大n-gram长度（通常 $N=4$）

  - $w_n$ 是第 $n$ 个n-gram的权重（通常 $w_n = \frac{1}{N}$）

  - $p_n$ 是n-gram**精确度**：即预测文本中的正确的n-gram所占比例

    



### ROUGE 分数

**ROUGE**（Recall-Oriented Understudy for Gisting Evaluation）是一组用于评估**自动摘要**或**翻译质量**的指标，通过比较生成摘要与人类参考摘要之间的重叠情况来衡量质量。

- 主要关注**召回率**：候选摘要覆盖了多少参考摘要的内容
- 支持多种匹配粒度：n-gram、词序列、词汇等
- 适用于摘要评估、文档生成等任务

其主要变体包括：

- **ROUGE-N**（n-gram 重叠）
- **ROUGE-L**（最长公共子序列 LCS）

2.1 ROUGE-N (n-gram共现统计)

- 召回率公式
  $$
  \text{ROUGE-N}_{\text{recall}} = \frac{\sum_{S \in \{\text{RefSummaries}\}} \sum_{\text{gram}_n \in S} \text{Count}_{\text{match}}(\text{gram}_n)}{\sum_{S \in \{\text{RefSummaries}\}} \sum_{\text{gram}_n \in S} \text{Count}(\text{gram}_n)}
  $$

- 精确率公式
  $$
  \text{ROUGE-N}_{\text{precision}} = \frac{\sum_{S \in \{\text{RefSummaries}\}} \sum_{\text{gram}_n \in S} \text{Count}_{\text{match}}(\text{gram}_n)}{\sum_{\text{gram}_n \in \text{CandSummary}} \text{Count}(\text{gram}_n)}
  $$

- F1分数
  $$
  \text{ROUGE-N}_{F1} = \frac{2 \times \text{ROUGE-N}_{\text{precision}} \times \text{ROUGE-N}_{\text{recall}}}{\text{ROUGE-N}_{\text{precision}} + \text{ROUGE-N}_{\text{recall}}}
  $$
  
  
  

其中：

- $\text{Count}_{\text{match}}(\text{gram}_n)$ 是在候选摘要和参考摘要中都出现的n-gram数量
- $\text{Count}(\text{gram}_n)$ 是n-gram的总数量



### ROUGE-L (最长公共子序列)

基于最长公共子序列（LCS）的ROUGE分数：

- LCS召回率
  $$
  R_{lcs} = \frac{\text{LCS}(X, Y)}{m}
  $$

- LCS精确率
  $$
  P_{lcs} = \frac{\text{LCS}(X, Y)}{n}
  $$
  
- LCS F分数
  $$
  F_{lcs} = \frac{(1 + \beta^2) R_{lcs} P_{lcs}}{R_{lcs} + \beta^2 P_{lcs}}
  $$
  

其中：

- $X$ 是长度为 $m$ 的参考摘要序列
- $Y$ 是长度为 $n$ 的候选摘要序列
- $\text{LCS}(X, Y)$ 是最长公共子序列的长度
- $\beta$ 控制精确率和召回率的权重（通常 $\beta = 1$）



## 19. Left padding 与 Right padding

- **结论**: **训练**时一般使用 **right padding**，**推理**的时候一般使用**left padding**
- **训练**：原理上使用left，right没有区别，因为可以设置ignore_idx忽略掉padding位置的loss计算；但是某些huggingface某些tokenizer使用的是absolute position id，即如果left padding会导致非padding开始token的position id不是0。
- **推理**：由于LLM是decoder only架构，生成的时候基于**next token prediction**进行生成，如果使用 right padding会导致结尾的padding token影响生成任务。



## DPO & PPO & GRPO

- DPO：
  - 没有 `reward model`与 `critical model`，直接对偏好数据对进行建模
  -  训练时候只有便好数据对，没有多种回答采样
  - off-policy
- PPO
  - 采用 `reward model`与 `critical model`进行优势函数建模
  - 加载四个模型，更新两个模型参数，资源消耗大
  - 可以基于多种采样回答进行训练(roll out)
  - on-policy
- GRPO
  - 去掉`reward model`与 `critical model`训练
  - 直接使用奖励函数计算奖励值
  - 基于组件采样计算优势函数值
  - 可以基于多种采样回答进行训练(roll out)



### RLHF & PPO

PPO 总共涉及到四个模型：如下图所示:

<img src="https://s2.loli.net/2025/03/04/2iY9hZ8kU1Hr6Mj.png" alt="image-20250304171048192" style="zoom:67%;" />

- 其中需要训练(更新参数的是 **Policy(Actor) Model** 和 **Value(Critic) Model)**

- 需要先基于便好数据对训练Reward Model

  - 模型输入一个句子(prompt + answer)，输出一个分数。

  - 通过pair wise loss对模型进行训练，对一个pair的句子，对好的句子打分高，对坏的句子打分低。
    $$
    \text{loss} \mathrel{+}= -\frac{1}{N} \sum_{i=1}^{N} \log\left(\sigma(\hat{r}_i - r_i)\right)
    $$

    ```python
    loss += -torch.log(torch.sigmoid(c_truncated_reward - r_truncated_reward)).mean()
    ```

    - 该模型输出两个值: 一个标量代表及时`reward`值：对输入的`prompt + answer`进行打分;一个向量`(B,L)`,第`i`个位置代表从该位置到最后每个位置的累积奖励值(状态函数值)，为后续`critic model`使用。

      ```python
      # 奖励模型返回的是个字典，key为chosen_end_scores位置存储数据维度为(B,)，表示对于prompt+answer的打分
      reward_score = self.reward_model.forward_value(
                      seq, attention_mask,
                      prompt_length=self.prompt_length)['chosen_end_scores'].detach(
                      )
      #critic model返回的数据维度为(B,L)，L维度上第i个位置代表从i位置到最后的累积奖励
      #舍去最后一个位置是因为句子“终止符”无意义 
      values = self.critic_model.forward_value(
                      seq, attention_mask, return_value_only=True).detach()[:, :-1]
      ```

- 为了防止模型输出的内容 `hack reward`，将`policy model`在`prompt`下输出的`answer`拼接在一起分别输入到 `policy model` 和 `ref model`中获取到`answer`位置的概率值，然后计算`KL散度`进行约束。

- 对于 `answer`序列(`prompt`不需要计算状态函数值)的每个位置的 `reward`更新为上述的 `KL散度`，并且在最后一个位置加上整个 `prompt + answer`的打分值 。

- 根据GAE(Generalized Advantage Estimation)优势函数计算优势值。
  $$
  \begin{align}
  A_t^{\text{GAE}(\gamma, \lambda)} &= \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}^V \\
  \delta_t^V &= r_t + \gamma V(s_{t+1}) - V(s_t)
  \end{align}
  $$

- 冻结`value model`，使用PPO/PPO2的优化目标公式，计算损失，梯度更新，优化`policy model`:
  $$
  \begin{align}
  
  J^{\theta^k}{\text{PPO}}(\theta) &= E_{(s_t, a_t) \sim \pi_{\theta'}} \left[ \frac{p_\theta(a_t \mid s_t)}{p_{\theta'}(a_t \mid s_t)} A^{\theta'}(s_t, a_t) - \beta \, KL(\theta, \theta') \right]\\
  
  
  J^{{\theta^k}}_\text{PPO2}(\theta) &\approx \sum_{(s_t, a_t)} \min \left( \frac{p_\theta(a_t | s_t)}{p_{\theta^k}(a_t | s_t)} A^{\theta^k}(s_t, a_t), \ 
  \text{clip} \left( \frac{p_\theta(a_t | s_t)}{p_{\theta^k}(a_t | s_t)}, 1 - \epsilon, 1 + \epsilon \right) A^{\theta^k}(s_t, a_t) \right)
  
  
  \end{align}
  $$
  
  
  - 直观上看当优势值大于0时，增加此时token的生成概率；直观上看当优势值小于0时，减小此时token的生成概率。
  - 计算重要性采样系数:其中旧的`policy model`需要几个 `epoch`才会更新一次。
  
  
  
- 根据 TD 差分法优化 `critic model`，其中损失为`mse loss`
  $$
  \mathcal{L}_V = \frac{1}{2} \mathbb{E}_t \left[ \left( V(s_t) - (r_t + \gamma V(s_{t+1})) \right)^2 \right]
  $$

  - $ V(s_t)$ 是当前状态的价值估计，
  - $r_t + \gamma V(s_{t+1})$ 是目标值（通过即时奖励和下一状态的价值估计计算得到的目标。
  - 计算目标值冻结梯度，更新`critic model`参数让当前状态的价值估计拟合目标值



### DPO-直接偏好优化

**通过利用奖励函数与最优策略之间的映射关系，证明这个受限的奖励最大化问题可以通过单阶段的策略训练来精确优化，本质上是在人类偏好数据上解决一个分类问题**

- **不需要单独根据便好数据训练reward model**，**直接使用偏好数据对**(一个query，有一个chosen回答和一个reject回答的数据集)进行目标函数建模实现**增加chosen样本($y_w$)的对数概率**，**减小reject样本($y_l$)的对数概率**

- **通过与reference model的概率比值**，限制模型更新过偏移reference model，学习到某种特定的策略实现最大化奖励导致reward hacking。

- 优化目标 & 求导: 
  $$
  \mathcal{L}{\text{DPO}}(\pi\theta; \pi_{\text{ref}}) = -\mathbb{E}{(x,y_w,y_l)\sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi\theta(y_w | x)}{\pi_{\text{ref}}(y_w | x)} - \beta \log \frac{\pi_\theta(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \right) \right]
  $$

  $$
  \nabla_\theta \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = 
  -\beta \mathbb{E}_{(x,y_w,y_l)\sim \mathcal{D}} \left[ 
  \underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w))}_{\text{higher weight when reward estimate is wrong}} 
  \left[ 
  \underbrace{\nabla_\theta \log \pi(y_w | x)}_{\text{increase likelihood of } y_w} - 
  \underbrace{\nabla_\theta \log \pi(y_l | x)}_{\text{decrease likelihood of } y_l} 
  \right] 
  \right]
  $$

  

  - 隐式奖励函数：
    $$
    \hat{r}\theta(x, y) = \beta \log \frac{\pi\theta(y | x)}{\pi_{\text{ref}}(y | x)}
    $$

- 直观解释:

  - 优化DPO直接在偏好数据对上增加偏好数据的对数概率，减小非便好数据的对数概率

  - 通过隐式奖励函数和缩放超参 $\beta$ 建立隐式奖励模型作为**梯度加权系数**, 代表隐式奖励模型认为**策略模型错误的程度**

  - ref model作用: 相当于引入了一个**细粒度的Value Model**和一个**细粒度的[Reward Model](https://zhida.zhihu.com/search?content_id=249317057&content_type=Article&match_order=1&q=Reward+Model&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NTY0NDk0MTQsInEiOiJSZXdhcmQgTW9kZWwiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoyNDkzMTcwNTcsImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.mNdQgyyNz7NAaVCI61fIlsEZjXUfZJ4VzLJkneBCxaw&zhida_source=entity)**，等价于PPO中Advantage的计算
    $$
    \beta \log \frac{\pi^*(a_t|s_t)}{\pi_{\text{ref}}(a_t|s_t)} = r(s_t, a_t) + V^*(s_{t+1}) - V^*(s_t)
    $$


### GRPO

与PPO对比如图：

<img src="https://s2.loli.net/2025/02/24/a64urQnPjqBhegG.png" alt="image.jpeg" style="zoom:67%;" />

- 去掉了`reward model`的训练，使用基于规则的奖励函数，例如格式正确，结果正确等等

- 去掉了`critical model`，使用Group采样的方式计算优势函数值

- 

  - 具体来说对于每个`query`，GRPO 从旧策略 $π_{θold}$ 中采样一组输出 $[{o_1，o_2，…，o_G}]$，通过奖励函数计算每个采样的奖励制，通过均值归一化公式计算优势函数值:
    $$
    A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \ldots, r_G\})}{\text{std}(\{r_1, r_2, \ldots, r_G\})}
    $$

  - 其余优化目标与PPO相同

- 深度解析

  - policy model优化目标的梯度为：
    $$
    \nabla_\theta J^{\theta^k}{\text{PPO}}(\theta) = E{(s_t, a_t) \sim \pi_{\theta'}} \left[ \frac{p_\theta(a_t \mid s_t)}{p_{\theta'}(a_t \mid s_t)} \nabla_\theta \log p_\theta(a_t \mid s_t) A^{\theta'}(s_t, a_t) - \beta , \nabla_\theta KL(\theta, \theta') \right]
    $$
    
  - 由于优势函数的计算是**基于整个序列的reward**，因此理论上一个序列中的每个token的优势值$A$是相同的，具有稀疏性，此时梯度的主要共享是前面的重要性采样系数
  
    - 如果该系数为1，那么不考虑KL散度的情况下**每个token就只剩下了相同的优势值，其实这样就退化成了SFT训练的交叉熵梯度，也就是每个token是具有相同奖励的行为克隆**
    - **从强化学习策略梯度定理角度来看，RL相对于SFT梯度公式最大的区别在于每个动作（token）是具备不同优势值的。**
  
- 缺点

  - **熵坍塌：**强化学习一直强调的“探索”与“利用”中的“探索”，“探索”在强化学习训练中是十分重要的，如果模型想获得更有的策略决策能力，也就是**在不同状态下需要去选择更优的动作就需要去“探索”新的动作带来的收益**，因为强化学习训练本身就是**一个通过不断试错来找到最优解的过程**，如果在强化学习训练过程中让策略模型（也就是Actor/Policy模型）没有足够的“探索”能力（只在少量的输出中训练，这样使得模型输出分布变得越来越尖锐，模型分布会变得集中，分布的熵会变小）那模型就不会找的更好的路径，就会造成所谓的“熵坍塌”。
  - **训练不稳定**：训练过程中梯度更新波动，无法收敛
  - **奖励噪声：**由于GRPO极度依赖奖励函数，GRPO中对过长的回答设置了长度惩罚奖励会导致正确的回答因为长度过长被惩罚；并且对于大多数实际任务来说，奖励函数是需要多方面权衡的，很难找到一个标准化的奖励函数去做奖励判断；奖励函数可能会引来噪声干扰训练
  - **序列级别建模**：优势函数的计算是**基于整个序列的reward**，但是**概率的优化是逐token**的，即 $\log p_\theta(a_t \mid s_t)$是对单个token的生成概率的优化





### DAPO

**解耦裁剪和动态采样策略优化（DAPO）算法**，针对GRPO提出了4个技术进行优化

- **裁剪偏移**（[Clip-Shifting](https://zhida.zhihu.com/search?content_id=242916111&content_type=Article&match_order=1&q=Clip-Shifting&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NTY5NjcwNjMsInEiOiJDbGlwLVNoaWZ0aW5nIiwiemhpZGFfc291cmNlIjoiZW50aXR5IiwiY29udGVudF9pZCI6MjQyOTE2MTExLCJjb250ZW50X3R5cGUiOiJBcnRpY2xlIiwibWF0Y2hfb3JkZXIiOjEsInpkX3Rva2VuIjpudWxsfQ.d4pObU_RuMzwUjuMBMtjFJekgyLnJXJaUi4eCg4Bi6Y&zhida_source=entity)）：DAPO中直接去掉了KL散度，增强RL过程中的探索能力；使用$PPO_2$中的clip方法进行约束：并且对clip中的**上下界使用不同的阈值**：**上界$\epsilon_{high}$大**，**下界$\epsilon_{low}$小 鼓励模型进行探索学习，防止熵坍塌**
  $$
  \text{clip} \left( \frac{p_\theta(a_t | s_t)}{p_{\theta^k}(a_t | s_t)}, 1 - \epsilon, 1 + \epsilon \right) A^{\theta^k}(s_t, a_t)
  $$
  

  - 如果clip函数的上界限不高，则重要性采样系数会被限制的不高，参考**policy model优化目标的梯度公式**，模型会更加注重高概率token部分的更新，低概率token的共享偏低，缺乏探索熵坍塌(分布变得集中)。
  - $\epsilon_{low}$也不能过大，否则也会出现熵坍塌
  - 举个例子：比如旧策略采样到某个token的概率是0.9，按照裁剪上限1.2计算$(1 + \epsilon)$，则新策略采样到该token的概率是接近于1（0.9\*1.2 但最大为1），注意新策略是我们训练目标函数期望的概率分布，也就是说**旧策略中本身高概率的那些token是不容易被上限裁剪的**，哪怕新策略下这个token采样概率很高了也容易被裁剪。反之当旧策略采样到某个token概率是0.1时（一般情况下低概率token不容易被采样到，但强化训练Rollout会具备一定随机性），如果同样的现在裁剪上限是1.2，那么新策略下这个token最高的采样概率也就是0.1*1.2=0.12，也就是说对于**旧策略概率低的token，即使训练后这个token的采样概率也不会有很大提升，因为提升上限被裁剪限制了**。这样一对比，（0.99-0.9）>> (0.12-0.1)是不是就很明显的看到差距了，这也是为什么DAPO要提升裁剪上限，因为不这样做的话，本来旧策略模型采样**概率高的token会随着训练变得采样概率越来越高，而低的token只会有很小的提升**，**那么结果就是模型输出的分布越来越尖锐，也就使得分布的熵变低，造成熵坍塌现象。**

- **动态采样（Dynamic Sampling）**：设置了采样结果的范围 :$[0,G]$比如$G=32$，意思是限制一组采样结果中正确的个数大于0，并且小于$G$；也就是采样的结果不能全错也不能全对，保证多样性，提高训练稳定性

- **溢出奖励塑造**[Overflowing Reward Shaping]：GRPO中对长序列的裁剪直接进行惩罚，引入了奖励噪声；采用**软性超长惩罚机制**：对于超过最大值的长度的序列设置-1奖励，设置一个缓存区间用于捕捉长序列中回答正确但是长度过长的问题，在这个区间使用线性的惩罚:
  $$
  R_{\text{length}}(y) = \begin{cases}
  0, & |y| \leq L_{\max} - L_{\text{cache}} \\
  \frac{(L_{\max} - L_{\text{cache}}) - |y|}{L_{\text{cache}}}, & L_{\max} - L_{\text{cache}} < |y| \leq L_{\max} \\
  -1, & L_{\max} < |y|
  \end{cases}
  $$
  
- **Token级策略梯度损失**（Token-Level Policy Gradient Loss）：GRPO中对优化目标的中损失建模过程是先对一个回答中的每个token求平均，再对所有回答求平均，这会导致长序列的约束减弱；DAPO中对所有的回答的所有token的优化目标损失求均值
  $$
  \begin{align}
  \text{GRPO:} \quad &\frac{1}{G} \sum_{i=1}^{G} \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \\
  \text{DAPO:} \quad &\frac{1}{\sum_{i=1}^{G} |o_i|} \sum_{i=1}^{G} \sum_{t=1}^{|o_i|}
  \end{align}
  $$

  - 举例：假设 $G$ 大小为2，**一个序列token长度为100**，这个序列的优势值是1，**另一个token序列长度为10**，这个序列的优势值为-1，为简单起见不考虑裁剪，重要性采样修正的操作。**如果使用GRPO计算，则最后的结果为0**（此时只代表目标函数值标量为0，但反向传播是看的梯度，梯度一般不为0），而如果使用DAPO计算则结果为$\frac{90}{110}$，在计算过程中能感受到如果使用**GRPO的方法是没有考虑token粒度**的，也就是说在同一个样本中每个token所占的权重系数随着token长度的增加而减小，这样的话模型输出**序列的长度越长，每个token在训练中对应的概率调整的幅度就越小。**

  

### GSPO

GRPO的重要性采样参数修正粒度不对: **GRPO中是对序列中每个token进行的重要性采样，因为动作的粒度是token(逐token计算概率)，但是奖励却是对整个序列的奖励，这样会造成一种逻辑冲突的问题。**

- GSPO将重要性采样系数调整为了序列的维度: 优势值的计算仍然是在一组回答的序列维度进行计算，这里$s_i(\theta)$代表的**是一个序列中每个token的重要性采样累乘后的值**进行**几何平均**，也就是说括号中的重要性采样参数（注意的下缀中不再有$t$）是这个序列中每个token的**重要性采样修正项的乘积结果**。
  $$
  \begin{align}
  \mathcal{J}_{\text{GSPO}}(\theta) &= \mathbb{E}_{x \sim \mathcal{D}, \{y_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|x)} \left[ \frac{1}{G} \sum_{i=1}^G \min \left( s_i(\theta) \hat{A}_i, \text{clip}(s_i(\theta), 1 - \varepsilon, 1 + \varepsilon) \hat{A}_i \right) \right] \\
  \text{其中：}
  \hat{A}_i &= \frac{r(x, y_i) - \text{mean}\left(\{r(x, y_i)\}_{i=1}^G\right)}{\text{std}\left(\{r(x, y_i)\}_{i=1}^G\right)} \\
  s_i(\theta) &= \left( \frac{\pi_\theta(y_i | x)}{\pi_{\theta_{\text{old}}}(y_i | x)} \right)^{\frac{1}{|y_i|}} = \exp \left( \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} \log \frac{\pi_\theta(y_{i,t} | x, y_{i,<t})}{\pi_{\theta_{\text{old}}}(y_{i,t} | x, y_{i,<t})} \right)
  \end{align}
  $$
  

## RLHF中 为什么不对 loss 直接进行梯度下降来求解

- RLHF的目标:
  $$
  \max_{\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(\cdot|x)} [r_{\phi}(x, y)] - \beta \text{KL}(\pi_{\theta}(\cdot|x) || \pi_{\text{ref}}(\cdot|x))
  $$

- 如果直接将“奖励当损失”，把策略输出的序列$y$接在奖励模型前面，一起端到端反传存在以下问题:

  -  序列$y$是通过 `argmax`, `beam search`等操作进行，这些操作是不可导的。

  - 通过策略梯度定理(REINFORCE)：不直接对 $r(y)$ 进行求导，而是通过对策略函数 $\pi_{\theta}$ 生成序列$y$的概率，对$r(y)$的期望进行建模，从而对这个概率进行求导。
    $$
    \nabla_{\theta} \mathbb{E}_{y \sim \pi_{\theta}} [r(y)] = \mathbb{E}_{y \sim \pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(y) \cdot r(y)]
    $$
    
    $$
    
    $$
    

## MOE

- 

- 多专家:替换单个FFN为多个专家。

- 门控路由网络:输出一个分布，选取topk进行激活对应专家

  <img src="https://s2.loli.net/2025/08/28/4boMqQz1EjhKNZd.png" alt="图片" style="zoom: 50%;" />

## CLIP

1. 首先通过两个模态的encoder获取对应的模态embedding
2. 然后进行norm2归一化(用于计算余弦相似度)
3. 对归一化后的embedding进行矩阵乘法获取logits
4. 计算对称的nce_loss

```python
import torch
import torch.nn as nn

class CLIP():
    def __init__(self,config):
        super.__init__(self,config)
        ...
    
    def encode_img(self,img)->torch.Tensor:
        ...
    
    def encode_txt(self,txt)->torch.Tensor:
        ...
       
    def nce_loss(self,logits:torch.Tensor,temperature:float):
        
        label = torch.arange(logits.shape[0],device=logits.device)
        logits = logits / temperature
        
        criterial = nn.CrossEntropyLoss()
        loss = criterial(logits,label)
        return loss
        
    def forward(self,img,txt):
        img_emb = self.encode_img(img)
        txt_emb = self.encode_txt(txt)
        
        img_emb = img_emb / img_emb.norm(dim=-1)
        txt_emb = txt_emb / txt_emb.norm(dim=-1)
        
        logits_img = torch.matmul(img_emb,txt_emb.T) #n,n
        
        loss = (self.nce_loss(logits_img,1) + self.nce_loss(logits_img.T,1)) / 2
        return loss
    
```



## Attention

### 原理

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$

####  1. 为什么需要处以$\sqrt{d}$：

1. 随着特征向量维度的增大，${QK^T}$ 计算的值的方差会变大(假设每个维度为独立同分布，则方差和维度长度成正比)，分布过于集中导致经过 `sofrmax`后退化成了 `argmax`即某个位置为1，其他位置都是0，导致反向传播过程中梯度消失。

2. 通过处以$\sqrt{d}$后，在维度上平均了特征向量的方差，使得分布变得更加均匀；相当于一个温度系数，提高温度系数(维度 $d$ >> 1 )

   

### 实现

1. 初始化`hidden_size`，`head_num`, `head_dim`,`wq`,`wk`,`wv`,`wo`
2. 首先计算`query`,`key`,`value`值，并调整shape为`(bsz,head_num,seq,head_dim)`:此时张量不连续
3. 拼接KV Cache：保持`key`,`value`张量的序列长度
4. 计算未归一化的`attention scores`
5. 计算`padding mask`
6. `softmax`归一化 `attention scores`计算得到 `attention probs`
7. 计算attention之后的``context value`,并调整为 `(bsz,seq,hidden_size)`: 注意从非连续张量  `view`为连续张量

```python
class Attention(nn.Module):
    def __init__(self,config):
        super.__init__()
        self.hidden_size = config.hidden_size
        self.head_num = config.head_num
        
        if(self.hidden_size % self.head_num != 0):
            raise Exception("WRONG HEAD NUM")

        self.head_dim = self.hidden_size // self.head_num
        
        self.wq = nn.Linear(self.hidden_size,self.hidden_size)
        self.wk = nn.Linear(self.hidden_size,self.hidden_size)  
        self.wv = nn.Linear(self.hidden_size,self.hidden_size)
        self.wo = nn.Linear(self.hidden_size,self.hidden_size)     
        
    
    def forward(self,query,key,value,mask,past_key_value):
        bsz = query.shape[0]
        seq = query.shape[1]
        
        query = self.wq(query).view(bsz,seq,self.head_num,self.head_dim).transpose(1,2)
        key = self.wk(key).view(bsz,seq,self.head_num,self.head_dim).transpose(1,2)
        value = self.wv(value).view(bsz,seq,self.head_num,self.head_dim).transpose(1,2)
        
        if(past_key_value is not None):
            key = torch.cat((past_key_value[0],key),dim=2)
            value = torch.cat((past_key_value[1],value),dim=2)
        past_key_value = (key,value)
        
        atten_scores = torch.matmul(query,key.transpose(-1,-2)) / torch.sqrt(self.head_dim)
        if(mask is not None):
            mask = mask.unsqueeze(1).unsqueeze(2)
            mask = (1-mask) * (-torch.inf)
            atten_scores = atten_scores + mask
        
        atten_probs = nn.functional.softmax(atten_scores,dim=-1)   
        
        context = torch.matmul(atten_probs,value).transpose(1,2).contiguous().view(bsz,seq,self.hidden_size)
        context = self.wo(context)
  
        return context,past_key_value     
  
```



## LLaMa

### LLaMa1

- 去重,过滤低质量数据等操作

- Pre-Norm+RMSNorm (激活函数/残差)
  
- 采用SwiGLU激活函数


- 采用RoPE旋转位置编码:

- 采用KV cache

### LLaMa 2

- 上下文长度 -> **4096**
- GQA 替换传统的 muti-head attention

### LLaMa 3

- 上下文长度 -> 8192
- 换成openai的tokenizer：tiktoken

### LLaMa 3.1

- 上下文长度 -> 128k
- 405B超大杯模型
- 多语言模型



## Deepspeed

1. 参数分割策略

   1. **流水线并行（Pipeline Parallel,PP）**：按照模型的**层（Layer）进行分割**，保留每一层（Layer）为整体，不同层存储在不同的 GPU 中， 多个层（GPU）串行在一起，需要串行执行。
      1. 时间效率很差， 并且如果某一层的参数量就很大并超过了单卡的显存就尴尬。
      2. 可以通过异步执行一定程度解决时间效率差的问题。
   2. **张量并行（Tensor Parallel,TP）**:
      1.  把参数张量切开，切开张量分开存储
      2. 但切开之后，张量计算的时候可以分两种策略。
         1.  张量的计算过程也是可以切割，这样把一个大的张量，切分成多个小张量，**每张 GPU 卡只保存一个小片段**，**每个小张量片段（GPU卡）独立进行相关计算**，最后在需要的时候合并结果就行了 Megatron 就是走的这个路线。
         2. 同样是把参数张量分割，**每张卡只保存一个片段。但是需要计算的时候，每张卡都从其他卡同步其它片段过来**，恢复完整的参数张量，再继续数据计算。**Deepspeed** 选取的这个策略，**这个策略实现起来更简单一些。**

2. 模型并行: **流水线并行**，**张量并行**(不同的参数分割策略)。
   1. 把模型一次完整的计算过程（前后向）分拆到多个 GPU 上进行。

3. 数据并行:pytorch 的 Data Parallel (DP) 和 Distributed Data Parallel (DDP)
   1. 每张卡都能进行模型一次完整前后向计算，只是每张卡处理不同的训练数据批次（batch）。

4. deepspeed 对参数进行了分割，每张卡存储一个片段，**但在进行运算时， 每张卡都会恢复完整的参数张量**，每张卡处理不同的数据批次， 因此 deepspeed **属于数据并行**。

5.  deepspeed不同级别机制
   1. ZeRO-0：禁用所有类型的分片，仅使用 DeepSpeed 作为 DDP (Distributed Data Parallel)
   2. ZeRO-1：**分割Optimizer States，减少了4倍的内存**，通信容量与数据并行性相同
   3. ZeRO-2：**分割Optimizer States与Gradients，内存减少8倍**，通信容量与数据并行性相同
   4. ZeRO-3：**分割Optimizer States、Gradients与 Model Parameters**，内存减少与数据并行度和复杂度成线性关系。
   5. ZeRO-Infinity：是ZeRO-3的拓展。允许通过使用 **NVMe 固态硬盘扩展** GPU 和 CPU 内存来训练大型模型。ZeRO-Infinity 需要启用 ZeRO-3。

6. DeepSpeed提供了多种优化策略和工具,其中ZeRO和Offload是两种重要的技术,它们有以下主要区别:
   1. 优化目标:
      1. ZeRO (Zero Redundancy Optimizer) ：主要目标是减少模型状态(参数、梯度、优化器状态)的内存占用。
      
      2. Offload：则专注于将部分计算和内存负载从**GPU转移到CPU或NVMe上。**
      
         

## Qwen-VL系列

### Qwen-VL-Chat

- LLM：Qwen-7B

- ViT：Openclip: ViT-bigG(1.9b)，

  1. 使用一个**适配器**压缩图像的token序列长度:一个固定序列长度的(256)可学习的**query**向量的cros-attention模块，图像序列作为**Key**。**将图像序列压缩到固定的256。**
  2. 可以感知位置信息，**2d绝对位置编码。位置编码向量维度切割成一半**，代表2d的坐标数量。**每一半单独做一个1d绝对位置编码。**
  
  <img src="https://s2.loli.net/2025/03/07/kdy4ap6tnOjvUuP.png" alt="image-20250307163319761" style="zoom:50%;" />
  
  
  
- 训练

  - 第一步冻结LLM训练 **ViT与适配器** (PT)
  - 第二步训练 **LLM+ViT+适配器** (PT)
  - 第三部训练 **适配器+LLM** (SFT)

### Qwen2-VL

1. 更小的视觉编码器: 675M

2. 更多的模型尺寸: 2B,7B,72B

3. 去掉了**适配器**，采用原生动态分辨率。为了减少token，采用**MLP层进行merge，将2*2的 patch token压缩为1个token。**

4. **图文数据与视频数据混合训练**

5. 采用深度为2的3D卷积，图片会复制一张，但是没有增加图片的patch即token数量，而是相当于将两张图片叠在一起**增加了图片每个patch token的embedding维度**

6. 设计了多模态旋转位置嵌入

   - 2d: 二维坐标获得位置编码，两两一组变为4,4一组

   - 3d:三维坐标获得位置编码,6-6一组进行旋转

     <img src="https://s2.loli.net/2025/03/07/nOAbmH3p9sVI7de.png" alt="image-20250307164852924" style="zoom: 50%;" />

- 训练
  - 训练**ViT+MLP**，冻结LLM（PT）
  - 训练**所有** （PT）
  - 训练**LLM与MLP**连接层。

###  Qwen-2.5-VL

- 与LLaMa对齐:RMSNorm，SwiGLU激活函数

- window attention:只在一个window中做attention而非全局序列进行attention，有效减少了 ViT 端的计算负担，提高了训练和推理速度。

- 绝对时间对齐: 动态FPS采样
  - 将视频三维位置**id与绝对时间id对齐**，**而不是与帧序列索引对齐**。
  
  - 同一帧索引id在不同的FPS下代表的是不同的时间，与时间对齐不存在这个问题
  
  - 假设视频长度为 `8s`，`temperal_merging_size = 2`, `tokens_per_second = 2`
  
    - 在0.5fps采样，采取到4帧图像，通过merge后为2帧图像
      - 采用原始索引编码：[0,1]
      - 采用与时间id对齐编码: [0,15] (首位帧采样+均匀分布)
    - 在1fps采样，采取到8帧图像，通过merge后为4帧图像
      - 采用原始索引编码：[0,1,2,3]
      - 采用与时间id对齐编码: [0,5,10,15] (首位帧采样+均匀分布)
    - 在2fps采样，采取到16帧图像，通过merge后为8帧图像
      - 采用原始索引编码：[0,1,2,3,4,5,6,7]
      - 采用与时间id对齐编码: [0,2,4,6,9,11,13,15] (首位帧采样+均匀分布)
  
    ![image-20250828003109263](https://s2.loli.net/2025/08/28/6op7YqgStLAdNTh.png)





